# A Comprehensive Survey of Document-level Relation Extraction (2016-2023) - Ph√¢n t√≠ch ph·∫£n bi·ªán ƒë·∫ßy ƒë·ªß

## Th√¥ng tin b√†i b√°o

- **Ti√™u ƒë·ªÅ**: A Comprehensive Survey of Document-level Relation Extraction (2016-2023)
- **T√°c gi·∫£**: Julien Delaunay, Hanh Thi Hong Tran, Carlos-Emiliano Gonz√°lez-Gallardo, Georgeta Bordea, Nicolas Sidere, Antoine Doucet (Univ. La Rochelle, L3i, France)
- **C√¥ng b·ªë**: ACM (n·ªôp th√°ng 10/2023)
- **DOI**: 10.1145/nnnnnnn.nnnnnnn (pending)
- **Tr√≠ch d·∫´n**: N/A (qu√° m·ªõi)
- **Lƒ©nh v·ª±c nghi√™n c·ª©u**: Natural Language Processing, Information Extraction, Document-level Relation Extraction
- **Lo·∫°i t√†i li·ªáu**: Survey/Literature Review (KH√îNG ph·∫£i nghi√™n c·ª©u th·ª±c nghi·ªám)

---

## TL;DR

Kh·∫£o s√°t t·ªïng quan n√†y h·ªá th·ªëng h√≥a 80+ ph∆∞∆°ng ph√°p v√† 10+ b·ªô d·ªØ li·ªáu cho tr√≠ch xu·∫•t quan h·ªá ·ªü m·ª©c t√†i li·ªáu (DocRE) giai ƒëo·∫°n 2016-2023, theo d√µi s·ª± ti·∫øn h√≥a t·ª´ c√°c c√°ch ti·∫øp c·∫≠n d·ª±a tr√™n chu·ªói sang d·ª±a tr√™n ƒë·ªì th·ªã r·ªìi d·ª±a tr√™n transformer. **K·∫øt lu·∫≠n ch√≠nh**: C√°c m√¥ h√¨nh ƒë·ªì th·ªã (68.72% F1) v∆∞·ª£t c√°c m√¥ h√¨nh transformer (65.87% F1) tr√™n benchmark DocRED, nh∆∞ng c√≥ **khi·∫øm khuy·∫øt nghi√™m tr·ªçng**: kh√¥ng c√≥ ki·ªÉm ƒë·ªãnh √Ω nghƒ©a th·ªëng k√™, kh√¥ng c√≥ ƒëo l∆∞·ªùng chi ph√≠ t√≠nh to√°n, v√† kh√¥ng c√≥ m·ªëc chu·∫©n hi·ªáu nƒÉng c·ªßa con ng∆∞·ªùi.

---

## C√¢u h·ªèi nghi√™n c·ª©u & gi·∫£ thuy·∫øt

| RQ | C√¢u h·ªèi | K·∫øt qu·∫£ | Ch·∫•t l∆∞·ª£ng |
|----|--------|---------|------------|
| **SRQ1** | Nh·ªØng ngu·ªìn l·ª±c (b·ªô d·ªØ li·ªáu) n√†o t·ªìn t·∫°i cho DocRE? | ‚úì **ƒê√£ tr·∫£ l·ªùi**: 10+ b·ªô d·ªØ li·ªáu ƒë∆∞·ª£c li·ªát k√™ (B·∫£ng 1-2) | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ |
| **SRQ2** | Nh·ªØng h∆∞·ªõng ti·∫øp c·∫≠n n√†o t·ªìn t·∫°i cho DocRE? | ‚úì **ƒê√£ tr·∫£ l·ªùi**: 80+ ph∆∞∆°ng ph√°p, x√¢y d·ª±ng ph√¢n lo·∫°i (H√¨nh 4) | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ |
| **SRQ3** | C√°c ph∆∞∆°ng ph√°p so s√°nh v·ªõi nhau v·ªÅ hi·ªáu nƒÉng nh∆∞ th·∫ø n√†o? | ‚ö†Ô∏è **M·ªôt ph·∫ßn**: C√≥ B·∫£ng 3-6, nh∆∞ng **kh√¥ng c√≥ ki·ªÉm ƒë·ªãnh th·ªëng k√™** | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ |
| **SRQ4** | C√°c h·∫°n ch·∫ø & h∆∞·ªõng nghi√™n c·ª©u t∆∞∆°ng lai l√† g√¨? | ‚úì **ƒê√£ tr·∫£ l·ªùi**: Th·∫£o lu·∫≠n ·ªü M·ª•c 8 | ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ |

**ƒê√°nh gi√° t·ªïng th·ªÉ ch·∫•t l∆∞·ª£ng RQ**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ - C√°c RQ ph√π h·ª£p v·ªõi t·ªïng quan t√†i li·ªáu, nh∆∞ng SRQ3 ƒë∆∞·ª£c x·ª≠ l√Ω ch∆∞a th·ªèa ƒë√°ng (kh√¥ng c√≥ th·ªëng k√™ suy lu·∫≠n).

---

## Khung l√Ω thuy·∫øt

### N·ªÅn t·∫£ng

**DocRE nh∆∞ ph·∫ßn m·ªü r·ªông c·ªßa tr√≠ch xu·∫•t quan h·ªá m·ª©c c√¢u (Sentence-level RE)**:
- **M·ª©c c√¢u**: 2 th·ª±c th·ªÉ, m·ªói th·ª±c th·ªÉ 1 l·∫ßn nh·∫Øc, quan h·ªá tuy·∫øn t√≠nh
- **M·ª©c t√†i li·ªáu**: 10-30+ th·ª±c th·ªÉ, nhi·ªÅu l·∫ßn nh·∫Øc (ƒë·ªìng tham chi·∫øu), quan h·ªá b·∫≠c hai n(n-1)

**K√Ω hi·ªáu h√¨nh th·ª©c** (Zhang et al. 2020):
```
Document: D = {S_i}_{i=1}^{n_s} (n_s sentences)
Entity Set: V = {E_i}_{i=1}^{n_e} (n_e entities)
Entity: E_i = {m_j}_{j=1}^{n_i^m} (multiple mentions)
Goal: Predict R' ‚äÜ R (all inter/intra-sentence relations)
```

### ƒê√≥ng g√≥p m·ªõi

**C√°c ti·∫øn tri·ªÉn kh√°i ni·ªám ƒë∆∞·ª£c ghi nh·∫≠n**:
1. **Bi·ªÉu di·ªÖn l·∫•y th·ª±c th·ªÉ l√†m trung t√¢m** (Jia 2019): avg ‚Üí max ‚Üí **logsumexp** (g·ªôp m·ªÅm)
2. **Ng∆∞·ª°ng th√≠ch nghi** (ATLOP 2020): Ng∆∞·ª°ng theo t·ª´ng c·∫∑p cho ph√¢n lo·∫°i ƒëa nh√£n
3. **G·ªôp ng·ªØ c·∫£nh c·ª•c b·ªô** (ATLOP 2020): Embedding th·ª±c th·ªÉ kh√°c nhau t√πy ƒë·ªëi t√°c
4. **Ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a cho DocRE** (DocuNet 2021): K·ªπ thu·∫≠t th·ªã gi√°c m√°y t√≠nh ‚Üí NLP

**C∆° s·ªü l√Ω thuy·∫øt**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ
- H√¨nh th·ª©c h√≥a v·ªØng (Zhang et al. 2020)
- Th·ªÉ hi·ªán r√µ ti·∫øn tr√¨nh c√°c m√¥ h√¨nh ki·∫øn tr√∫c
- Li√™n h·ªá v·ªõi l√Ω thuy·∫øt b·ªô nh·ªõ l√†m vi·ªác (Barreyro 2012, Cowan 2001) cho gi·ªõi h·∫°n b·∫±ng ch·ª©ng 3 c√¢u

---

## T·ªïng quan ph∆∞∆°ng ph√°p

### ƒê√°nh gi√° ph∆∞∆°ng ph√°p kh·∫£o s√°t

| Kh√≠a c·∫°nh | Th√¥ng l·ªá chu·∫©n | Kh·∫£o s√°t n√†y | Kho·∫£ng tr·ªëng | M·ª©c ƒë·ªô |
|----------|----------------|--------------|--------------|--------|
| **Giao th·ª©c t√¨m ki·∫øm** | PRISMA, Cochrane | Phi ch√≠nh th·ª©c (Google Scholar, ACL) | Cao | üü° |
| **Ti√™u ch√≠ ch·ªçn l·ªçc** | Khung PICO r√µ r√†ng | "Li√™n quan ƒë·∫øn DocRE" | **Nghi√™m tr·ªçng** | üî¥ |
| **ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng** | C√¥ng c·ª• ƒë√°nh gi√° thi√™n l·ªách | ‚ùå Kh√¥ng th·ª±c hi·ªán | **Nghi√™m tr·ªçng** | üî¥ |
| **ƒê·ªô tin c·∫≠y gi·ªØa ng∆∞·ªùi ƒë√°nh gi√°** | Cohen's Œ∫, Krippendorff's Œ± | ‚ùå Kh√¥ng b√°o c√°o | Cao | üü° |
| **Tr√≠ch xu·∫•t d·ªØ li·ªáu** | Bi·ªÉu m·∫´u ƒë·ªãnh s·∫µn | ‚ùå Ng·∫ßm ƒë·ªãnh | V·ª´a | üü° |
| **T·ªïng h·ª£p** | Meta-analysis (n·∫øu kh·∫£ thi) | T∆∞·ªùng thu·∫≠t + b·∫£ng | V·ª´a | üü° |
| **Thi√™n l·ªách c√¥ng b·ªë** | Bi·ªÉu ƒë·ªì ph·ªÖu, ki·ªÉm ƒë·ªãnh Egger | ‚ùå Kh√¥ng ƒë√°nh gi√° | V·ª´a | üü° |

**ƒê·ªô ch·∫∑t ch·∫Ω ph∆∞∆°ng ph√°p t·ªïng th·ªÉ**: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (2/5)

**K·∫øt lu·∫≠n**: **T·ªïng quan t∆∞·ªùng thu·∫≠t to√†n di·ªán, KH√îNG ph·∫£i systematic review** theo chu·∫©n y khoa/l√¢m s√†ng.

**T√°c ƒë·ªông**:
- ‚úì Bao qu√°t b·ªÅ r·ªông lƒ©nh v·ª±c
- ‚úó C√≥ th·ªÉ c√≥ thi√™n l·ªách ch·ªçn l·ªçc (t√≠nh ch·ªß quan c·ªßa "li√™n quan")
- ‚úó Kh√¥ng th·ªÉ ƒë·ªãnh l∆∞·ª£ng d·ªã bi·ªát (heterogeneity)
- ‚úó Kh·∫£ nƒÉng t√°i l·∫≠p h·∫°n ch·∫ø (kh√¥ng c√≥ s∆° ƒë·ªì PRISMA)

---

## T√≥m t·∫Øt c√°c ph√°t hi·ªán ch√≠nh

### Ph√°t hi·ªán ch√≠nh 1: B·ª©c tranh b·ªô d·ªØ li·ªáu

**Ph√°t hi·ªán**: Thi·∫øu c√°c b·ªô d·ªØ li·ªáu g√°n nh√£n v√†ng (gold) quy m√¥ l·ªõn; chu·∫©n ƒë√°nh gi√° b·ªã ‚Äúƒë·ªôc canh‚Äù quanh DocRED.

| Mi·ªÅn | B·ªô d·ªØ li·ªáu gold | L·ªõn nh·∫•t (N t√†i li·ªáu) | V·∫•n ƒë·ªÅ |
|------|------------------|------------------------|--------|
| **T·ªïng qu√°t** | 3 (DocRED, Re-DocRED, DWIE) | DocRED (5,053) | Thi√™n l·ªách g√°n nh√£n, √¢m t√≠nh gi·∫£ |
| **Y sinh** | 2 (BC5CDR, BioRED) | BC5CDR (1,500) | Quy m√¥ nh·ªè, chuy√™n bi·ªát |
| **Silver** | Nhi·ªÅu (GDA, DocRED-distant) | DocRED-distant (101,873) | Nhi·ªÖu (distant supervision) |

**M·ª©c ƒë·ªô b·∫±ng ch·ª©ng**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (ƒê∆∞·ª£c tr√¨nh b√†y t·ªët ·ªü B·∫£ng 1-2)

**V·∫•n ƒë·ªÅ then ch·ªët**:
1. **Thi√™n l·ªách DocRED** (Huang 2022, Tan 2022b):
   - G√°n nh√£n ki·ªÉu recommend-revise ‚Üí √¢m t√≠nh gi·∫£ c√≥ t√≠nh h·ªá th·ªëng
   - Thi·∫øu tri th·ª©c KB ‚Üí thi·∫øu quan h·ªá
   - Re-DocRED b·ªï sung +81% quan h·ªá (12.5 ‚Üí 28.1 trung b√¨nh)
2. **ƒê·ªôc canh benchmark**: 40+ ph∆∞∆°ng ph√°p tr√™n DocRED so v·ªõi **2 ph∆∞∆°ng ph√°p tr√™n DWIE**

**ƒê·ªô v·ªØng**: ‚úì ƒê∆∞·ª£c x√°c th·ª±c b·ªüi c√°c nghi√™n c·ª©u t√°i g√°n nh√£n ƒë·ªôc l·∫≠p (Re-DocRED)

---

### Ph√°t hi·ªán ch√≠nh 2: ƒê·ªì th·ªã > Transformer (ƒê∆Ø·ª¢C TUY√äN B·ªê)

**Tuy√™n b·ªë**: "Graph-based methods [...] are more performant than transformer-based ones" (p.25)

**B·∫±ng ch·ª©ng** (DocRED, BERT-base):
- **ƒê·ªì th·ªã t·ªët nh·∫•t**: EGCN-BERT (68.72 F1, 65.95 IgnF1) [Sun et al. 2022b]
- **Transformer t·ªët nh·∫•t**: DREEAM (65.87 F1, 63.73 IgnF1) [Ma et al. 2023]
- **Ch√™nh l·ªách**: 2.85 ƒëi·ªÉm F1

**H·ªó tr·ª£ th·ªëng k√™**: ‚ùå **B·∫∞NG KH√îNG**
- Kh√¥ng c√≥ ki·ªÉm ƒë·ªãnh √Ω nghƒ©a (t-test, McNemar's, Wilcoxon)
- Kh√¥ng c√≥ kho·∫£ng tin c·∫≠y (bootstrap, ph√¢n t√≠ch)
- Kh√¥ng c√≥ error bars
- Kh√°c nhau v·ªÅ random seeds, si√™u tham s·ªë ‚Üí kh√¥ng th·ªÉ lo·∫°i tr·ª´ bi·∫øn thi√™n do l·∫•y m·∫´u

**M·ª©c ƒë·ªô b·∫±ng ch·ª©ng**: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (Y·∫øu - ch·ªâ mang t√≠nh m√¥ t·∫£)

**C√°c gi·∫£i th√≠ch thay th·∫ø**:
1. **Bi·∫øn thi√™n do l·∫•y m·∫´u**: 2.85% c√≥ th·ªÉ n·∫±m trong nhi·ªÖu
2. **T·ªëi ∆∞u si√™u tham s·ªë**: M√¥ h√¨nh ƒë·ªì th·ªã c√≥ th·ªÉ ƒë∆∞·ª£c t·ªëi ∆∞u nhi·ªÅu h∆°n
3. **Overfitting benchmark**: EGCN ƒë∆∞·ª£c thi·∫øt k·∫ø ri√™ng cho DocRED
4. **Nhi·ªÖu do y·∫øu t·ªë nhi·ªÖu (confounds)**: Kh√°c bi·∫øn th·ªÉ BERT, kh√°c train/dev splits

**T√°i l·∫≠p (replication)**: ‚úó **Kh√¥ng ƒë∆∞·ª£c th·ª≠** (kh√¥ng c√≥ ki·ªÉm ch·ª©ng li√™n b·ªô d·ªØ li·ªáu tr√™n DWIE, BioRED)

---

### Ph√°t hi·ªán ch√≠nh 3: Chi ph√≠ t√≠nh to√°n (ƒê∆Ø·ª¢C TUY√äN B·ªê)

**Tuy√™n b·ªë**: "Graph-based methods [...] come with substantial computational overhead" (p.25)

**B·∫±ng ch·ª©ng**: ‚ùå **Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªãnh l∆∞·ª£ng**
- Kh√¥ng c√≥ ƒëo th·ªùi gian ch·∫°y (wall-clock, GPU hours)
- Kh√¥ng c√≥ s·ª≠ d·ª•ng b·ªô nh·ªõ (RAM, VRAM)
- Kh√¥ng c√≥ ph√¢n t√≠ch FLOPs
- Kh√¥ng c√≥ throughput (docs/sec)
- Kh√¥ng c√≥ ti√™u th·ª• nƒÉng l∆∞·ª£ng

**M·ª©c ƒë·ªô b·∫±ng ch·ª©ng**: ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ (Kh√¥ng c√≥)

**K·∫øt lu·∫≠n**: üö© **C·ªú ƒê·ªé** - Nh·∫≠n ƒë·ªãnh ho√†n to√†n kh√¥ng ƒë∆∞·ª£c ch·ª©ng minh

**T√°c ƒë·ªông**: **L·ªói nghi√™m tr·ªçng** - ng∆∞·ªùi th·ª±c h√†nh kh√¥ng th·ªÉ ra quy·∫øt ƒë·ªãnh chi ph√≠‚Äìl·ª£i √≠ch

**ƒêi·ªÅu c·∫ßn c√≥**:
- Bi√™n Pareto hi·ªáu nƒÉng‚Äìchi ph√≠ c√≥ th·ª±c nghi·ªám
- Th·ªùi gian ch·∫°y tr√™n ph·∫ßn c·ª©ng chu·∫©n (V100, A100)
- Chi ph√≠ tr√™n m·ªói 1000 t√†i li·ªáu
- Ablation: C·∫•u tr√∫c ƒë·ªì th·ªã mang l·∫°i ƒë·ªß gi√° tr·ªã ƒë·ªÉ b√π chi ph√≠ hay kh√¥ng?

---

### Ph√°t hi·ªán ch√≠nh 4: Suy lu·∫≠n ƒëa c√¢u

**Tuy√™n b·ªë**: "40.7% of DocRED relations require multi-sentence reasoning" (p.8, Yao 2019)

**B·∫±ng ch·ª©ng**:
- **Nh·∫≠n d·∫°ng m·∫´u** (trong c√¢u): 38.9%
- **Suy lu·∫≠n ƒëa c√¢u**: 40.7%
  - Logic (th·ª±c th·ªÉ c·∫ßu n·ªëi): 26.6%
  - ƒê·ªìng tham chi·∫øu: 17.6%
  - Tri th·ª©c th∆∞·ªùng th·ª©c: 16.6%

**B·∫±ng ch·ª©ng h·ªó tr·ª£** (Huang 2021):
- 96.1% quan h·ªá: ‚â§3 c√¢u
- 87.7% quan h·ªá: ‚â§2 c√¢u

**C∆° s·ªü l√Ω thuy·∫øt**: H·∫°n ch·∫ø b·ªô nh·ªõ l√†m vi·ªác (Barreyro 2012), gi·ªõi h·∫°n 3-4 ‚Äúm·∫£nh‚Äù (Cowan 2001)

**M·ª©c ƒë·ªô b·∫±ng ch·ª©ng**: ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ (V·ª´a)

**H·∫°n ch·∫ø**:
- **Ch·ªâ m·ªôt b·ªô d·ªØ li·ªáu**: ch·ªâ DocRED (ƒëo·∫°n Wikipedia)
- **G√°n nh√£n ch·ªß quan**: ph√¢n lo·∫°i ki·ªÉu suy lu·∫≠n th·ªß c√¥ng
- **Ch∆∞a ƒë∆∞·ª£c x√°c th·ª±c**: kh√¥ng c√≥ t√°i l·∫≠p tr√™n DWIE, BioRED, hay mi·ªÅn kh√°c

**Kh·∫£ nƒÉng kh√°i qu√°t**: Ch∆∞a r√µ 40.7% c√≥ ƒë√∫ng v·ªõi b√†i b√°o khoa h·ªçc, vƒÉn b·∫£n ph√°p l√Ω, m·∫°ng x√£ h·ªôi hay kh√¥ng

---

### Ph√°t hi·ªán ch√≠nh 5: Tr·∫ßn hi·ªáu nƒÉng

**Ph√°t hi·ªán**: SOTA hi·ªán t·∫°i (EGCN-BERT): 68.72% F1 tr√™n DocRED

**Ti·∫øn b·ªô**: +15% F1 t·ª´ 2016-2023
- Baseline 2019 (Bi-LSTM): 51.06 F1
- 2020 GAIN (+10%): 61.24 F1
- 2020 ATLOP: 61.30 F1
- 2022 EGCN (+6%): 68.72 F1
- 2023 DREEAM: 65.87 F1

**M·ª©c ƒë·ªô b·∫±ng ch·ª©ng**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (Ti·∫øn tr√¨nh ƒë∆∞·ª£c tr√¨nh b√†y r√µ)

**Kho·∫£ng tr·ªëng nghi√™m tr·ªçng**: ‚ùå **Kh√¥ng c√≥ m·ªëc hi·ªáu nƒÉng c·ªßa con ng∆∞·ªùi**
- F1 c·ªßa con ng∆∞·ªùi tr√™n DocRED l√† bao nhi√™u?
- M·ª©c nh·∫•t qu√°n gi·ªØa ng∆∞·ªùi g√°n nh√£n (Cohen's Œ∫) l√† bao nhi√™u?
- 68% g·∫ßn tr·∫ßn hay v·∫´n c√≤n nhi·ªÅu d∆∞ ƒë·ªãa?

**B·ªëi c·∫£nh so s√°nh**:
- RE m·ª©c c√¢u: ~90% F1 (TAC-RED)
- NER: ~95% F1 (CoNLL-2003)
- DocRE: 68.72% F1 (**k√©m 20-25%**)

**H√†m √Ω**: B√†i to√°n ho·∫∑c r·∫•t kh√≥, ho·∫∑c m√¥ h√¨nh c√≤n xa t·ªëi ∆∞u

---

## ƒêi·ªÉm m·∫°nh

### ƒêi·ªÉm m·∫°nh v·ªÅ ph∆∞∆°ng ph√°p

1. **H·ªá th·ªëng h√≥a to√†n di·ªán**: 80+ ph∆∞∆°ng ph√°p, 10+ b·ªô d·ªØ li·ªáu - nhi·ªÅu kh·∫£ nƒÉng bao ph·ªß ƒë·∫ßy ƒë·ªß 2016-2023
2. **Ph√¢n lo·∫°i r√µ r√†ng**: Sequence ‚Üí Graph ‚Üí Transformer ‚Üí Semantic Segmentation (H√¨nh 4)
3. **Ph√¢n t√≠ch theo th·ªùi gian**: Theo d√µi ti·∫øn h√≥a t·ªët (H√¨nh 2 cho th·∫•y tƒÉng tr∆∞·ªüng theo h√†m m≈©)
4. **Li√™n k·∫øt m√£ ngu·ªìn**: URL GitHub cho ph·∫ßn l·ªõn ph∆∞∆°ng ph√°p ‚Üí h·ªó tr·ª£ t√°i l·∫≠p
5. **B·∫£ng so s√°nh**: Tra c·ª©u thu·∫≠n ti·ªán (B·∫£ng 3-6)

### ƒêi·ªÉm m·∫°nh v·ªÅ l√Ω thuy·∫øt

1. **Ph√¢n t√≠ch b·ªô d·ªØ li·ªáu c√≥ t√≠nh ph·∫£n bi·ªán**: Ch·ªâ ra thi√™n l·ªách DocRED (recommend-revise, thi·∫øu KB)
2. **Khuy·∫øn ngh·ªã ƒëa d·∫°ng h√≥a**: Th√∫c ƒë·∫©y d√πng DWIE, BioRED, Re-DocRED
3. **Theo d√µi ti·∫øn h√≥a kh√°i ni·ªám**: Bi·ªÉu di·ªÖn th·ª±c th·ªÉ (avg ‚Üí max ‚Üí logsumexp)
4. **H√¨nh th·ª©c h√≥a b√†i to√°n**: K√Ω hi·ªáu r√µ r√†ng (Zhang et al. 2020)
5. **Li√™n h·ªá khoa h·ªçc nh·∫≠n th·ª©c**: L√Ω thuy·∫øt b·ªô nh·ªõ l√†m vi·ªác cho gi·ªõi h·∫°n 3 c√¢u

### ƒêi·ªÉm m·∫°nh th·ª±c ti·ªÖn

1. **G·∫Øn v·ªõi ·ª©ng d·ª•ng**: X√¢y d·ª±ng KB, y sinh IE
2. **Khuy·∫øn ngh·ªã benchmark**: H∆∞·ªõng d·∫´n r√µ (d√πng Re-DocRED, kh√¥ng ch·ªâ DocRED)
3. **H∆∞·ªõng t∆∞∆°ng lai**: LLMs, semantic segmentation

---

## H·∫°n ch·∫ø & ƒëe d·ªça t√≠nh h·ª£p l·ªá

### H·∫°n ch·∫ø nghi√™m tr·ªçng (üî¥)

1. **Kh√¥ng c√≥ ki·ªÉm ƒë·ªãnh √Ω nghƒ©a th·ªëng k√™**
   - **T√°c ƒë·ªông**: Kh√¥ng th·ªÉ k·∫øt lu·∫≠n ƒê·ªì th·ªã > Transformer
   - **M·ª©c ƒë·ªô**: üî¥ Nghi√™m tr·ªçng
   - **Kh·∫Øc ph·ª•c**: Ch·∫°y McNemar's test, bootstrap CI

2. **Kh√¥ng c√≥ d·ªØ li·ªáu chi ph√≠ t√≠nh to√°n**
   - **T√°c ƒë·ªông**: C·ª•m "substantial overhead" ho√†n to√†n kh√¥ng c√≥ c∆° s·ªü
   - **M·ª©c ƒë·ªô**: üî¥ Nghi√™m tr·ªçng
   - **Kh·∫Øc ph·ª•c**: ƒêo runtime, memory, FLOPs

3. **Kh√¥ng c√≥ m·ªëc hi·ªáu nƒÉng c·ªßa con ng∆∞·ªùi**
   - **T√°c ƒë·ªông**: Kh√¥ng th·ªÉ ƒë√°nh gi√° 68% F1 g·∫ßn tr·∫ßn hay c√≤n d∆∞ ƒë·ªãa
   - **M·ª©c ƒë·ªô**: üî¥ Nghi√™m tr·ªçng
   - **Kh·∫Øc ph·ª•c**: Nghi√™n c·ª©u IAA tr√™n DocRED (Cohen's Œ∫)

4. **Kh√¥ng c√≥ giao th·ª©c systematic review**
   - **T√°c ƒë·ªông**: Thi√™n l·ªách ch·ªçn l·ªçc, kh√≥ t√°i l·∫≠p
   - **M·ª©c ƒë·ªô**: üî¥ Nghi√™m tr·ªçng
   - **Kh·∫Øc ph·ª•c**: Tu√¢n th·ªß PRISMA

5. **ƒê·ªôc canh benchmark**
   - **T√°c ƒë·ªông**: Overfitting DocRED ƒëe d·ªça kh·∫£ nƒÉng kh√°i qu√°t
   - **M·ª©c ƒë·ªô**: üî¥ Nghi√™m tr·ªçng
   - **Kh·∫Øc ph·ª•c**: B·∫Øt bu·ªôc ƒë√°nh gi√° ƒëa b·ªô d·ªØ li·ªáu

### H·∫°n ch·∫ø v·ª´a (üü°)

6. **Thi√™n l·ªách ng√¥n ng·ªØ**: Ch·ªâ ti·∫øng Anh (th·ª´a nh·∫≠n p.28 nh∆∞ng kh√¥ng x·ª≠ l√Ω)
7. **B·ªè qua d·ªã bi·ªát**: Kh√°c seeds, splits, si√™u tham s·ªë gi·ªØa c√°c b√†i
8. **F1 l√† th∆∞·ªõc ƒëo duy nh·∫•t**: M·∫•t c√¢n b·∫±ng l·ªõp (quan h·ªá N/A tr·ªôi) c√≥ th·ªÉ g√¢y hi·ªÉu sai
9. **Thi√™n l·ªách mi·ªÅn**: Ch·ªâ t·ªïng qu√°t + y sinh (kh√¥ng c√≥ ph√°p l√Ω, t√†i ch√≠nh, m·∫°ng x√£ h·ªôi)
10. **Thi√™n l·ªách c√¥ng b·ªë**: Ch·ªâ b√†i ƒë√£ c√¥ng b·ªë (kh√¥ng c√≥ grey literature, kh√¥ng c√≥ funnel plots)
11. **ƒê·ªô tin c·∫≠y gi·ªØa ng∆∞·ªùi ƒë√°nh gi√°**: Ch·ªçn b√†i mang t√≠nh ch·ªß quan (kh√¥ng c√≥ Cohen's Œ∫)

### H·∫°n ch·∫ø nh·ªè (üü¢)

12. **Suy ƒëo√°n v·ªÅ LLM**: Kh·∫£o s√°t d·ª´ng ·ªü 2023 (ƒë·∫ßu th·ªùi k·ª≥ GPT-4) - c√°c nh·∫≠n ƒë·ªãnh c√≥ th·ªÉ ƒë√£ l·∫°c h·∫≠u
13. **DWIE √≠t ƒë∆∞·ª£c d√πng**: Ch·ªâ 2 ph∆∞∆°ng ph√°p d√πng l·ª±a ch·ªçn gold thay th·∫ø (thi√™n l·ªách theo ƒë·ªô m·ªõi?)

### ƒê√°nh gi√° t√≠nh h·ª£p l·ªá

| Lo·∫°i h·ª£p l·ªá | ƒê√°nh gi√° | ƒêe d·ªça ch√≠nh |
|------------|----------|--------------|
| **N·ªôi t·∫°i (Internal)** | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ | Thi√™n l·ªách ch·ªçn l·ªçc, thi√™n l·ªách c√¥ng b·ªë |
| **Ngo·∫°i suy (External)** | ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ | Thi√™n l·ªách ng√¥n ng·ªØ, ƒë·ªôc canh benchmark, thi√™n l·ªách mi·ªÅn |
| **K·∫øt lu·∫≠n th·ªëng k√™** | ‚≠ê‚òÜ‚òÜ‚òÜ‚òÜ | Kh√¥ng ki·ªÉm ƒë·ªãnh √Ω nghƒ©a, kh√¥ng CI, b·ªè qua d·ªã bi·ªát |
| **Kh√°i ni·ªám (Construct)** | ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ | Ch·ªâ d√πng F1, "overhead" kh√¥ng ƒëo l∆∞·ªùng |

**T√≠nh h·ª£p l·ªá t·ªïng th·ªÉ**: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ

---

## ƒê√°nh gi√° ph·∫£n bi·ªán

### ƒêi·ªÅu kh·∫£o s√°t l√†m r·∫•t t·ªët

1. ‚úì **H·ªá th·ªëng h√≥a to√†n di·ªán**: Bao qu√°t b·ªÅ r·ªông lƒ©nh v·ª±c (80+ ph∆∞∆°ng ph√°p)
2. ‚úì **Ph√¢n t√≠ch b·ªô d·ªØ li·ªáu c√≥ t√≠nh ph·∫£n bi·ªán**: N√™u thi√™n l·ªách DocRED, khuy·∫øn ngh·ªã l·ª±a ch·ªçn thay th·∫ø
3. ‚úì **Ph√¢n lo·∫°i r√µ r√†ng**: Ti·∫øn tr√¨nh Sequence ‚Üí Graph ‚Üí Transformer
4. ‚úì **G√≥c nh√¨n theo th·ªùi gian**: Ti·∫øn h√≥a 2016-2023 ƒë∆∞·ª£c theo d√µi t·ªët
5. ‚úì **T√≠nh h·ªØu d·ª•ng th·ª±c ti·ªÖn**: Link GitHub, b·∫£ng so s√°nh, g·∫Øn ·ª©ng d·ª•ng

### Nh·ªØng ƒëi·ªÉm c√≥ th·ªÉ c·∫£i thi·ªán m·∫°nh

#### L·ªói nghi√™m tr·ªçng

1. ‚úó **Kh√¥ng c√≥ ki·ªÉm ƒë·ªãnh √Ω nghƒ©a th·ªëng k√™**
   - **V·∫•n ƒë·ªÅ**: K·∫øt lu·∫≠n "Graph > Transformer" d·ª±a tr√™n ch√™nh 2.85 F1 m√† kh√¥ng c√≥ p-values
   - **T√°c ƒë·ªông**: C√≥ th·ªÉ d·∫´n d·∫Øt c·ªông ƒë·ªìng theo/hay tr√°nh m·ªôt ki·∫øn tr√∫c v√¨ k·∫øt lu·∫≠n ch∆∞a ƒë·ªß c∆° s·ªü
   - **Kh·∫Øc ph·ª•c**: Ch·∫°y McNemar's test, bootstrap 95% CI

2. ‚úó **Kh√¥ng c√≥ ƒëo l∆∞·ªùng chi ph√≠ t√≠nh to√°n**
   - **V·∫•n ƒë·ªÅ**: Nh·∫≠n ƒë·ªãnh "Substantial overhead" kh√¥ng c√≥ b·∫•t k·ª≥ d·ªØ li·ªáu n√†o
   - **T√°c ƒë·ªông**: Ng∆∞·ªùi th·ª±c h√†nh kh√¥ng th·ªÉ ra quy·∫øt ƒë·ªãnh chi ph√≠‚Äìl·ª£i √≠ch
   - **Kh·∫Øc ph·ª•c**: ƒêo runtime (GPU hours), memory (VRAM), throughput (docs/sec)

3. ‚úó **Kh√¥ng c√≥ m·ªëc hi·ªáu nƒÉng c·ªßa con ng∆∞·ªùi**
   - **V·∫•n ƒë·ªÅ**: Kh√¥ng bi·∫øt 68% F1 g·∫ßn tr·∫ßn hay v·∫´n c√≤n d∆∞ ƒë·ªãa
   - **T√°c ƒë·ªông**: Kh√¥ng r√µ b√†i to√°n ƒëang b√£o h√≤a hay kh√¥ng
   - **Kh·∫Øc ph·ª•c**: Nghi√™n c·ª©u IAA (Cohen's Œ∫, Krippendorff's Œ±)

4. ‚úó **Kh√¥ng c√≥ giao th·ª©c systematic review**
   - **V·∫•n ƒë·ªÅ**: Thi√™n l·ªách ch·ªçn l·ªçc, t√°i l·∫≠p h·∫°n ch·∫ø
   - **T√°c ƒë·ªông**: Kh√¥ng th·ªÉ ƒë·ªôc l·∫≠p ki·ªÉm ch·ª©ng quy·∫øt ƒë·ªãnh ch·ªçn b√†i
   - **Kh·∫Øc ph·ª•c**: Tu√¢n th·ªß PRISMA, c√¥ng b·ªë ti√™u ch√≠ ch·ªçn l·ªçc

#### L·ªói v·ª´a

5. ‚ö†Ô∏è **ƒê·ªôc canh benchmark**
   - **V·∫•n ƒë·ªÅ**: 40+ ph∆∞∆°ng ph√°p tr√™n DocRED so v·ªõi 2 tr√™n DWIE
   - **T√°c ƒë·ªông**: Overfitting v√†o ƒëo·∫°n Wikipedia
   - **Kh·∫Øc ph·ª•c**: B·∫Øt bu·ªôc ƒë√°nh gi√° ƒëa b·ªô d·ªØ li·ªáu (DocRED + DWIE + BioRED + Re-DocRED)

6. ‚ö†Ô∏è **Thi√™n l·ªách ng√¥n ng·ªØ**
   - **V·∫•n ƒë·ªÅ**: Ch·ªâ ti·∫øng Anh (th·ª´a nh·∫≠n nh∆∞ng kh√¥ng x·ª≠ l√Ω)
   - **T√°c ƒë·ªông**: Kh√¥ng kh√°i qu√°t cho DocRE ƒëa ng√¥n ng·ªØ
   - **Kh·∫Øc ph·ª•c**: Kh·∫£o s√°t c√°c c√¥ng tr√¨nh kh√¥ng ti·∫øng Anh (Chinese, Arabic, multilingual datasets)

### C√°c gi·∫£i th√≠ch thay th·∫ø cho ph√°t hi·ªán ch√≠nh

**V·ªÅ "Graph > Transformer"**:
1. **Bi·∫øn thi√™n do l·∫•y m·∫´u**: Ch√™nh 2.85 F1 c√≥ th·ªÉ n·∫±m trong nhi·ªÖu (c·∫ßn bootstrap CI)
2. **T·ªëi ∆∞u si√™u tham s·ªë**: M√¥ h√¨nh ƒë·ªì th·ªã c√≥ th·ªÉ ƒë∆∞·ª£c ƒë·∫ßu t∆∞ h∆°n (thi√™n l·ªách c√¥ng b·ªë)
3. **Overfitting benchmark**: EGCN thi·∫øt k·∫ø ri√™ng cho DocRED
4. **Y·∫øu t·ªë nhi·ªÖu**: Kh√°c bi·∫øn th·ªÉ BERT, kh√°c random seeds

**V·ªÅ "40.7% suy lu·∫≠n ƒëa c√¢u"**:
1. **T√≠nh ch·ªß quan c·ªßa g√°n nh√£n**: Ph√¢n lo·∫°i ki·ªÉu suy lu·∫≠n th·ªß c√¥ng
2. **ƒê·∫∑c th√π b·ªô d·ªØ li·ªáu**: C√≥ th·ªÉ kh√¥ng kh√°i qu√°t ngo√†i DocRED (ƒëo·∫°n Wikipedia)
3. **Danh m·ª•c ch·ªìng l·∫•n**: T·ªïng > 100% g·ª£i √Ω c√°c danh m·ª•c kh√¥ng lo·∫°i tr·ª´ nhau

### C√¢u h·ªèi ch∆∞a ƒë∆∞·ª£c tr·∫£ l·ªùi

**Nghi√™m tr·ªçng**:
1. F1 c·ªßa con ng∆∞·ªùi tr√™n DocRED l√† bao nhi√™u? (c·∫≠n tr√™n cho hi·ªáu nƒÉng m√¥ h√¨nh)
2. Inter-annotator agreement (Cohen's Œ∫) l√† bao nhi√™u?
3. M√¥ h√¨nh ƒë·ªì th·ªã c√≥ th·ª±c s·ª± ƒë·∫Øt h∆°n kh√¥ng? (c·∫ßn runtime, memory, FLOPs)
4. Ch√™nh 2.85 F1 c√≥ √Ω nghƒ©a th·ªëng k√™ kh√¥ng? (c·∫ßn p-values, CI)

**V·ª´a**:
5. ∆Øu th·∫ø ƒë·ªì th·ªã c√≥ kh√°i qu√°t qua c√°c b·ªô d·ªØ li·ªáu kh√¥ng? (DWIE, BioRED, Re-DocRED)
6. V√¨ sao DWIE √≠t ƒë∆∞·ª£c d√πng? (Ch·ªâ 2 ph∆∞∆°ng ph√°p d√π c√≥ gold annotation)
7. Bi√™n Pareto chi ph√≠‚Äìhi·ªáu nƒÉng l√† g√¨? (hi·ªáu qu·∫£ vs ƒë·ªô ch√≠nh x√°c)

**ThƒÉm d√≤**:
8. LLMs (GPT-4, LLaMA-3, Claude-3.5) c√≥ thay th·∫ø m√¥ h√¨nh DocRE chuy√™n bi·ªát kh√¥ng?
9. Semantic segmentation c√≥ th·ªÉ ƒë∆∞·ª£c khai th√°c th√™m kh√¥ng? (ch·ªâ 2 b√†i, k·∫øt qu·∫£ h·ª©a h·∫πn)
10. DocRE ƒëa ph∆∞∆°ng th·ª©c th√¨ sao? (t√†i li·ªáu + h√¨nh ·∫£nh/b·∫£ng)

---

## ƒê√°nh gi√° kh·∫£ nƒÉng t√°i l·∫≠p

### Checklist

- [x] M√¥ t·∫£ chi·∫øn l∆∞·ª£c t√¨m ki·∫øm (Google Scholar, Elsevier, ACL Anthology)
- [ ] ‚ùå Ti√™u ch√≠ ƒë∆∞a v√†o/lo·∫°i tr·ª´ r√µ r√†ng (ch·ªâ n√≥i "relevant to DocRE")
- [ ] ‚ùå S∆° ƒë·ªì PRISMA
- [ ] ‚ùå Chia s·∫ª bi·ªÉu m·∫´u tr√≠ch xu·∫•t d·ªØ li·ªáu
- [ ] ‚ùå Th·ª±c hi·ªán ƒë√°nh gi√° ch·∫•t l∆∞·ª£ng (kh√¥ng c√≥ risk-of-bias tool)
- [ ] ‚ùå B√°o c√°o ƒë·ªô ƒë·ªìng thu·∫≠n gi·ªØa ng∆∞·ªùi ƒë√°nh gi√° (kh√¥ng c√≥ Cohen's Œ∫)
- [ ] ‚ùå ƒê√°nh gi√° risk-of-bias
- [ ] ‚ùå ƒê√°nh gi√° thi√™n l·ªách c√¥ng b·ªë (kh√¥ng c√≥ funnel plots)
- [x] C√≥ b·∫£ng so s√°nh (B·∫£ng 3-6)
- [ ] ‚ùå Meta-analysis (kh√¥ng th·ª±c hi·ªán)
- [x] C√≥ link m√£ ngu·ªìn (URL GitHub cho h·∫ßu h·∫øt ph∆∞∆°ng ph√°p)

**ƒêi·ªÉm t√°i l·∫≠p**: ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ (2/5) - Th·∫•p‚ÄìTrung b√¨nh

**Kh·∫£ nƒÉng replication**: Trung b√¨nh
- C√≥ th·ªÉ t√¨m l·∫°i c∆° s·ªü d·ªØ li·ªáu v·ªõi c√πng thu·∫≠t ng·ªØ
- Nh∆∞ng **kh√¥ng th·ªÉ ƒë·ªôc l·∫≠p ki·ªÉm ch·ª©ng quy·∫øt ƒë·ªãnh ch·ªçn b√†i** (t√≠nh ch·ªß quan c·ªßa "relevance")

---

## ƒê√≥ng g√≥p cho lƒ©nh v·ª±c

### ƒê√≥ng g√≥p l√Ω thuy·∫øt

- ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ **N√¢ng cao hi·ªÉu bi·∫øt**: Ph√¢n lo·∫°i r√µ, theo d√µi ti·∫øn h√≥a kh√°i ni·ªám
- ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ **Gi·∫£i quy·∫øt tranh lu·∫≠n**: M·ªôt ph·∫ßn (ch·ªâ ra thi√™n l·ªách DocRED, nh∆∞ng kh√¥ng ph√¢n ƒë·ªãnh Graph vs Transformer)

### ƒê√≥ng g√≥p ph∆∞∆°ng ph√°p

- ‚≠ê‚≠ê‚≠ê‚òÜ‚òÜ **Ph∆∞∆°ng ph√°p kh·∫£o s√°t**: To√†n di·ªán nh∆∞ng kh√¥ng mang t√≠nh h·ªá th·ªëng (kh√¥ng tu√¢n PRISMA)
- ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ **Danh m·ª•c ngu·ªìn l·ª±c**: T√†i li·ªáu tham chi·∫øu quan tr·ªçng cho b·ªô d·ªØ li·ªáu, ph∆∞∆°ng ph√°p, link m√£ ngu·ªìn

### ƒê√≥ng g√≥p th·ª±c nghi·ªám

- ‚≠ê‚≠ê‚òÜ‚òÜ‚òÜ **Ph√¢n t√≠ch so s√°nh**: B·∫£ng h·ªØu √≠ch, nh∆∞ng thi·∫øu ch·∫∑t ch·∫Ω th·ªëng k√™ (kh√¥ng c√≥ ki·ªÉm ƒë·ªãnh √Ω nghƒ©a)
- ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ **Theo d√µi theo th·ªùi gian**: Ti·∫øn h√≥a 2016-2023 ƒë∆∞·ª£c ghi nh·∫≠n t·ªët

### H√†m √Ω th·ª±c ti·ªÖn

**V·ªõi nh√† nghi√™n c·ª©u**:
- ‚úì ƒêi·ªÉm kh·ªüi ƒë·∫ßu thi·∫øt y·∫øu cho DocRE
- ‚úì Ph√¢n lo·∫°i r√µ gi√∫p ƒë·ªãnh h∆∞·ªõng ph√°t tri·ªÉn ph∆∞∆°ng ph√°p
- ‚ö†Ô∏è Th·∫≠n tr·ªçng v·ªõi k·∫øt lu·∫≠n "Graph > Transformer" (kh√¥ng ƒë∆∞·ª£c x√°c th·ª±c th·ªëng k√™)

**V·ªõi ng∆∞·ªùi th·ª±c h√†nh**:
- ‚úì Bi·∫øt n√™n d√πng b·ªô d·ªØ li·ªáu n√†o (DocRED, Re-DocRED, DWIE, BioRED)
- ‚úó Kh√¥ng th·ªÉ ra quy·∫øt ƒë·ªãnh chi ph√≠‚Äìl·ª£i √≠ch (kh√¥ng c√≥ d·ªØ li·ªáu runtime)
- ‚ö†Ô∏è T·ª± ki·ªÉm ch·ª©ng nh·∫≠n ƒë·ªãnh v·ªÅ chi ph√≠ t√≠nh to√°n tr∆∞·ªõc khi tri·ªÉn khai

**T√°c ƒë·ªông t·ªïng th·ªÉ**: üî•üî•üî•üî•‚òÜ (T√°c ƒë·ªông cao)
- C√≥ kh·∫£ nƒÉng ƒë∆∞·ª£c tr√≠ch d·∫´n nhi·ªÅu (to√†n di·ªán, k·ªãp th·ªùi, t·ªï ch·ª©c t·ªët)
- Nh∆∞ng **kh√¥ng mang t√≠nh k·∫øt lu·∫≠n cu·ªëi c√πng** v√¨ thi·∫øu ch·∫∑t ch·∫Ω th·ªëng k√™

---

## V·ªã tr√≠ trong vƒÉn li·ªáu

### K·∫ø th·ª´a t·ª´

- **C√°c kh·∫£o s√°t RE tr∆∞·ªõc ƒë√¢y**:
  - Bach & Badaskar 2007 (ph∆∞∆°ng ph√°p gi√°m s√°t m·ª©c c√¢u)
  - Pawar et al. 2017 (m·ª©c c√¢u, CNN giai ƒëo·∫°n s·ªõm)
  - Bassignana & Plank 2022 (RE cho vƒÉn b·∫£n khoa h·ªçc, ch·ªâ nh·∫Øc DocRE ng·∫Øn)
- **Kho·∫£ng tr·ªëng ƒë∆∞·ª£c l·∫•p**: Kh·∫£o s√°t to√†n di·ªán ƒë·∫ßu ti√™n t·∫≠p trung ri√™ng cho DocRE

### ƒê∆∞·ª£c tr√≠ch d·∫´n b·ªüi (ti·ªÅm nƒÉng - qu√° m·ªõi)

- Nhi·ªÅu kh·∫£ nƒÉng tr·ªü th√†nh **t√†i li·ªáu tham chi·∫øu chu·∫©n** cho DocRE (to√†n di·ªán, k·ªãp th·ªùi)
- C√≥ th·ªÉ t√°c ƒë·ªông ƒë·∫øn s·ª± ƒëa d·∫°ng benchmark (khuy·∫øn ngh·ªã DWIE, BioRED, Re-DocRED)

### M√¢u thu·∫´n v·ªõi

- **M√¢u thu·∫´n ng·∫ßm**: C√°c kh·∫£o s√°t m·ª©c c√¢u tr∆∞·ªõc gi·∫£ ƒë·ªãnh pattern matching l√† ƒë·ªß
  - Kh·∫£o s√°t n√†y: 40.7% quan h·ªá DocRED c·∫ßn suy lu·∫≠n ƒëa c√¢u

---

## H∆∞·ªõng nghi√™n c·ª©u t∆∞∆°ng lai

### Do t√°c gi·∫£ g·ª£i √Ω

1. **LLMs cho DocRE**: th·ªùi k·ª≥ GPT-4, LLaMA-2 (kh·∫£o s√°t vi·∫øt 2023)
2. **Semantic segmentation n√¢ng cao**: K·ªπ thu·∫≠t th·ªã gi√°c m√°y t√≠nh ‚Üí NLP
3. **C·∫£i thi·ªán hi·ªáu qu·∫£**: M√¥ h√¨nh ƒë·ªì th·ªã n·∫∑ng t√≠nh to√°n (ƒë∆∞·ª£c tuy√™n b·ªë)
4. **K·∫øt h·ª£p NER+RE**: Multi-task learning
5. **V∆∞·ª£t ra ngo√†i named entities**: Terms, concepts (kh√¥ng ch·ªâ persons, orgs, locations)

### C√°c b∆∞·ªõc ti·∫øp theo then ch·ªët (ƒë√°nh gi√° c·ªßa t√¥i)

#### Ng·∫Øn h·∫°n (l·∫•p kho·∫£ng tr·ªëng nghi√™m tr·ªçng)

1. **Nghi√™n c·ª©u hi·ªáu nƒÉng con ng∆∞·ªùi**: Ch·∫°y IAA tr√™n DocRED (Cohen's Œ∫, Krippendorff's Œ±)
   - **V√¨ sao**: Thi·∫øt l·∫≠p c·∫≠n tr√™n cho hi·ªáu nƒÉng m√¥ h√¨nh
   - **T√°c ƒë·ªông**: Cao - ƒë√°nh gi√° b√†i to√°n c√≥ b√£o h√≤a hay kh√¥ng

2. **Ki·ªÉm ƒë·ªãnh √Ω nghƒ©a th·ªëng k√™**: Bootstrap CI cho m·ªçi so s√°nh m√¥ h√¨nh
   - **V√¨ sao**: Kh√¥ng th·ªÉ k·∫øt lu·∫≠n Graph > Transformer n·∫øu kh√¥ng c√≥ p-values
   - **T√°c ƒë·ªông**: Cao - x√°c th·ª±c/b√°c b·ªè tuy√™n b·ªë c·ªët l√µi

3. **Nghi√™n c·ª©u chi ph√≠ t√≠nh to√°n**: Runtime, memory, FLOPs cho Graph vs Transformer
   - **V√¨ sao**: Nh·∫≠n ƒë·ªãnh "substantial overhead" hi·ªán kh√¥ng c√≥ b·∫±ng ch·ª©ng
   - **T√°c ƒë·ªông**: Cao - cho ph√©p ph√¢n t√≠ch chi ph√≠‚Äìl·ª£i √≠ch

4. **Ki·ªÉm ch·ª©ng li√™n b·ªô d·ªØ li·ªáu**: ƒê√°nh gi√° c√°c ph∆∞∆°ng ph√°p t·ªët nh·∫•t tr√™n DWIE, BioRED, Re-DocRED
   - **V√¨ sao**: ƒê·ªôc canh benchmark ƒëe d·ªça kh·∫£ nƒÉng kh√°i qu√°t
   - **T√°c ƒë·ªông**: Cao - ki·ªÉm tra ƒë·ªô v·ªØng

#### Trung h·∫°n (m·ªü r·ªông ph·∫°m vi)

5. **DocRE ƒëa ng√¥n ng·ªØ**: Kh·∫£o s√°t c√¥ng tr√¨nh kh√¥ng ti·∫øng Anh (Chinese, Arabic)
   - **V√¨ sao**: Kh·∫£o s√°t hi·ªán ch·ªâ ti·∫øng Anh
   - **T√°c ƒë·ªông**: Trung b√¨nh - m·ªü r·ªông kh·∫£ nƒÉng √°p d·ª•ng

6. **M·ªü r·ªông mi·ªÅn**: Ph√°p l√Ω, t√†i ch√≠nh, m·∫°ng x√£ h·ªôi
   - **V√¨ sao**: Kh·∫£o s√°t hi·ªán ch·ªâ t·ªïng qu√°t + y sinh
   - **T√°c ƒë·ªông**: Trung b√¨nh - bao ph·ªß th·ª±c t·∫ø

7. **Benchmark LLM**: GPT-4, Claude-3.5, LLaMA-3 tr√™n DocRED
   - **V√¨ sao**: Kh·∫£o s√°t 2023 c√≥ tr∆∞·ªõc giai ƒëo·∫°n LLM b√£o h√≤a ƒë·∫ßy ƒë·ªß
   - **T√°c ƒë·ªông**: Cao - ƒë√°nh gi√° li·ªáu m√¥ h√¨nh chuy√™n bi·ªát c√≤n c·∫ßn thi·∫øt hay kh√¥ng

#### D√†i h·∫°n (ti·∫øn tri·ªÉn l√Ω thuy·∫øt)

8. **DocRE ƒëa ph∆∞∆°ng th·ª©c**: T√†i li·ªáu + h√¨nh ·∫£nh/b·∫£ng
   - **V√¨ sao**: T√†i li·ªáu th·ª±c t·∫ø th∆∞·ªùng ƒëa ph∆∞∆°ng th·ª©c
   - **T√°c ƒë·ªông**: Trung b√¨nh‚ÄìCao - h∆∞·ªõng m·ªõi

9. **Ph√¢n t√≠ch l√Ω thuy·∫øt**: PAC bounds, sample complexity cho DocRE
   - **V√¨ sao**: Hi·ªán ch∆∞a c√≥ b·∫£o ƒë·∫£m l√Ω thuy·∫øt
   - **T√°c ƒë·ªông**: Trung b√¨nh - c·ªßng c·ªë n·ªÅn t·∫£ng

10. **ƒê·ªô b·ªÅn tr∆∞·ªõc t·∫•n c√¥ng (adversarial robustness)**: Stress-test m√¥ h√¨nh DocRE
    - **V√¨ sao**: Kh√¥ng ƒë∆∞·ª£c b√†n trong kh·∫£o s√°t
    - **T√°c ƒë·ªông**: Trung b√¨nh - ƒë·ªô tin c·∫≠y

---

## C√°c ƒëi·ªÉm r√∫t ra ch√≠nh

### D√†nh cho nh√† nghi√™n c·ª©u NLP

1. **Ch√≠nh**: DocRE l√† lƒ©nh v·ª±c s√¥i ƒë·ªông (tƒÉng tr∆∞·ªüng theo h√†m m≈© 2016-2023), nh∆∞ng c·∫ßn:
   - ‚úó Ch·∫∑t ch·∫Ω th·ªëng k√™ (ki·ªÉm ƒë·ªãnh √Ω nghƒ©a)
   - ‚úó Ph√¢n t√≠ch chi ph√≠ t√≠nh to√°n
   - ‚úó M·ªëc chu·∫©n con ng∆∞·ªùi
2. **L∆∞u √Ω**: K·∫øt lu·∫≠n "Graph > Transformer" **kh√¥ng ƒë∆∞·ª£c x√°c th·ª±c th·ªëng k√™**
3. **C∆° h·ªôi**: Semantic segmentation c√≤n √≠t ƒë∆∞·ª£c khai th√°c (ch·ªâ 2 b√†i, k·∫øt qu·∫£ h·ª©a h·∫πn)
4. **Ngu·ªìn l·ª±c**: D√πng Re-DocRED, DWIE, BioRED (kh√¥ng ch·ªâ DocRED)

### D√†nh cho ng∆∞·ªùi th·ª±c h√†nh

1. **B·ªô d·ªØ li·ªáu**: DocRED (5K t√†i li·ªáu, t·ªïng qu√°t), Re-DocRED (t√°i g√°n nh√£n, +81% quan h·ªá), BioRED (y sinh)
2. **M√¥ h√¨nh**: EGCN-BERT (68.72% F1, SOTA ƒë·ªì th·ªã), DREEAM (65.87% F1, SOTA transformer)
3. **Th·∫≠n tr·ªçng**: Nh·∫≠n ƒë·ªãnh v·ªÅ chi ph√≠ t√≠nh to√°n ch∆∞a ƒë∆∞·ª£c ki·ªÉm ch·ª©ng - c·∫ßn ƒëo tr∆∞·ªõc khi tri·ªÉn khai
4. **ƒê√°nh ƒë·ªïi**: M√¥ h√¨nh ƒë·ªì th·ªã c√≥ th·ªÉ ch√≠nh x√°c h∆°n nh∆∞ng c√≥ th·ªÉ t·ªën k√©m h∆°n (c·∫ßn x√°c minh)

### D√†nh cho meta-researchers

1. **V√≠ d·ª•**: Kh·∫£o s√°t khoa h·ªçc m√°y t√≠nh th∆∞·ªùng thi·∫øu ƒë·ªô ch·∫∑t ch·∫Ω nh∆∞ systematic review y khoa (PRISMA)
2. **Kho·∫£ng tr·ªëng**: Ki·ªÉm ƒë·ªãnh √Ω nghƒ©a th·ªëng k√™ ch∆∞a th√†nh chu·∫©n trong vƒÉn li·ªáu kh·∫£o s√°t NLP
3. **C∆° h·ªôi**: Ph√°t tri·ªÉn h∆∞·ªõng d·∫´n systematic review ri√™ng cho NLP

---

## Ghi ch√∫ c√° nh√¢n

### ƒê·ªô tin c·∫≠y: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (4/5)

**C√≥ tin c√°c ph√°t hi·ªán?**:
- ‚úì **C√≥** cho: Danh m·ª•c b·ªô d·ªØ li·ªáu, ph√¢n lo·∫°i ph∆∞∆°ng ph√°p, ti·∫øn h√≥a theo th·ªùi gian, theo d√µi kh√°i ni·ªám
- ‚ö†Ô∏è **C√≥ ƒëi·ªÅu ki·ªán** cho: So s√°nh Graph vs Transformer (kh√¥ng c√≥ ki·ªÉm ƒë·ªãnh √Ω nghƒ©a)
- ‚úó **Kh√¥ng** cho: Nh·∫≠n ƒë·ªãnh chi ph√≠ t√≠nh to√°n (kh√¥ng c√≥ b·∫±ng ch·ª©ng)

**C√≥ khuy·∫øn ngh·ªã tr√≠ch d·∫´n?**:
- ‚úì **C√≥** - To√†n di·ªán, k·ªãp th·ªùi, t·ªï ch·ª©c t·ªët, c√≥ kh·∫£ nƒÉng tr·ªü th√†nh t√†i li·ªáu tham chi·∫øu chu·∫©n
- Nh∆∞ng n√™n **b·ªï sung**:
  - Ki·ªÉm ƒë·ªãnh th·ªëng k√™ n·∫øu so s√°nh m√¥ h√¨nh
  - Nghi√™n c·ª©u chi ph√≠ th·ª±c nghi·ªám n·∫øu tri·ªÉn khai

**M·ª©c ch·∫•t l∆∞·ª£ng**: **T·ªïng quan t∆∞·ªùng thu·∫≠t h·∫°ng cao** (nh∆∞ng KH√îNG ph·∫£i systematic review theo chu·∫©n y khoa)

### M·ª©c ƒë·ªô li√™n quan ƒë·∫øn nghi√™n c·ª©u DocRE

**N√™n ƒë·ªçc b·∫Øt bu·ªôc ƒë·ªëi v·ªõi**:
- Ng∆∞·ªùi m·ªõi v√†o DocRE (gi·ªõi thi·ªáu to√†n di·ªán)
- Ng∆∞·ªùi ph√°t tri·ªÉn ph∆∞∆°ng ph√°p (ph√¢n lo·∫°i r√µ, ch·ªâ ra kho·∫£ng tr·ªëng)
- Ng∆∞·ªùi t·∫°o benchmark (khuy·∫øn ngh·ªã ƒëa d·∫°ng h√≥a b·ªô d·ªØ li·ªáu)

**D√πng cho**:
- T·ªïng quan vƒÉn li·ªáu (danh m·ª•c to√†n di·ªán)
- ƒê·ªãnh v·ªã c√¥ng tr√¨nh (t√°c ph·∫©m c·ªßa t√¥i n·∫±m ·ªü ƒë√¢u?)
- So s√°nh baseline (B·∫£ng 3-6)

**Kh√¥ng n√™n d√πng cho**:
- K·∫øt lu·∫≠n th·ªëng k√™ v·ªÅ Graph vs Transformer (ch∆∞a ƒë∆∞·ª£c x√°c th·ª±c)
- ∆Ø·ªõc l∆∞·ª£ng chi ph√≠ t√≠nh to√°n (kh√¥ng ƒëo)
- C·∫≠n tr√™n hi·ªáu nƒÉng con ng∆∞·ªùi (kh√¥ng b√°o c√°o)

### C√°c b√†i n√™n ƒë·ªçc ti·∫øp

**N·ªÅn t·∫£ng**:
1. Yao et al. 2019 - DocRED dataset (thi·∫øt l·∫≠p benchmark)
2. Zhang et al. 2020 - DHG (h√¨nh th·ª©c h√≥a b√†i to√°n)
3. Zhou et al. 2020 - ATLOP (adaptive thresholding, localized pooling)

**SOTA**:
4. Sun et al. 2022b - EGCN (SOTA ƒë·ªì th·ªã hi·ªán t·∫°i, 68.72% F1)
5. Ma et al. 2023 - DREEAM (SOTA transformer hi·ªán t·∫°i, 65.87% F1)
6. Zhang et al. 2021 - DocuNet (semantic segmentation, √≠t ƒë∆∞·ª£c khai th√°c)

**Ph√¢n t√≠ch ph·∫£n bi·ªán**:
7. Huang et al. 2022 - Revisit DocRED (ph∆°i b√†y thi√™n l·ªách g√°n nh√£n)
8. Tan et al. 2022b - Re-DocRED (t√°i g√°n nh√£n, +81% quan h·ªá)

---

**Ng√†y ph√¢n t√≠ch**: 2026-01-29
**Ng∆∞·ªùi ph√¢n t√≠ch**: Ralph Reader Agent (Critical Reading Mode ‚ö†Ô∏è)
**Lo·∫°i b√†i**: Survey/Literature Review (KH√îNG ph·∫£i nghi√™n c·ª©u th·ª±c nghi·ªám)
**K·∫øt lu·∫≠n t·ªïng th·ªÉ**: ‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ (4/5) - **To√†n di·ªán v√† h·ªØu √≠ch, nh∆∞ng thi·∫øu ch·∫∑t ch·∫Ω th·ªëng k√™. T√†i li·ªáu tham chi·∫øu thi·∫øt y·∫øu d√π c√≤n khi·∫øm khuy·∫øt.**
