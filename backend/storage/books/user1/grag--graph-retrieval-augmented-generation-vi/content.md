# GRAG: Graph Retrieval-Augmented Generation - PhÃ¢n tÃ­ch phÃª bÃ¬nh hoÃ n chá»‰nh

**Paper ID**: 2025.findings-naacl.232
**Analysis Date**: 2026-01-29
**Analysis Mode**: Äá»c pháº£n biá»‡n (Äá»™ nghiÃªm ngáº·t cao)

---

## Paper Metadata

- **Title**: GRAG: Graph Retrieval-Augmented Generation
- **Authors**: Yuntong Hu, Zhihan Lei, Zheng Zhang, Bo Pan, Chen Ling, Liang Zhao
- **Affiliation**: Department of Computer Science, Emory University, Atlanta, GA
- **Published**: Findings of the Association for Computational Linguistics: NAACL 2025
- **Pages**: 4145â€“4157
- **Code**: https://github.com/HuieL/GRAG
- **Research Area**: NLP, Graph Reasoning, Retrieval-Augmented Generation

---

## TL;DR

**RQ**: LÃ m tháº¿ nÃ o Ä‘á»ƒ LLMs cÃ³ thá»ƒ sá»­ dá»¥ng hiá»‡u quáº£ cÃ¡c tÃ i liá»‡u cÃ³ tÃ­nh máº¡ng lÆ°á»›i (Ä‘á»“ thá»‹ vÄƒn báº£n) cho RAG?
**Method**: GRAG truy há»“i cÃ¡c tiá»ƒu Ä‘á»“ thá»‹ vÄƒn báº£n báº±ng chia-Ä‘á»ƒ-trá»‹ (ego-graphs + soft pruning), cung cáº¥p cho LLMs hai â€œgÃ³c nhÃ¬nâ€ (mÃ´ táº£ vÄƒn báº£n phÃ¢n cáº¥p + embeddings GNN).
**Main Finding**: GRAG tuyÃªn bá»‘ vÆ°á»£t cÃ¡c baseline RAG vÃ  cÃ¡c LLM Ä‘Æ°á»£c fine-tune trÃªn cÃ¡c benchmark há»i-Ä‘Ã¡p Ä‘á»“ thá»‹ (WebQSP, ExplaGraphs) â€” nhÆ°ng **KHÃ”NG cÃ³ kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a thá»‘ng kÃª** khiáº¿n cÃ¡c tuyÃªn bá»‘ khÃ´ng thá»ƒ kiá»ƒm chá»©ng.

**Bottom Line**: CÃ¡ch tiáº¿p cáº­n má»›i cho má»™t váº¥n Ä‘á» quan trá»ng, nhÆ°ng Ä‘á»™ nghiÃªm ngáº·t thá»‘ng kÃª thiáº¿u tráº§m trá»ng.

---

## Research Questions & Hypotheses

### Main Research Question
**RQ**: LÃ m tháº¿ nÃ o Ä‘á»ƒ LLMs cÃ³ thá»ƒ khai thÃ¡c hiá»‡u quáº£ cÃ¡c tÃ i liá»‡u cÃ³ tÃ­nh máº¡ng lÆ°á»›i (textual graphs) khi thá»±c hiá»‡n Retrieval-Augmented Generation (RAG)?

### Sub-questions
1. LÃ m tháº¿ nÃ o Ä‘á»ƒ truy há»“i hiá»‡u quáº£ cÃ¡c tiá»ƒu Ä‘á»“ thá»‹ vÄƒn báº£n liÃªn quan? (bÃ i toÃ¡n NP-hard)
2. LÃ m tháº¿ nÃ o Ä‘á»ƒ tÃ­ch há»£p thÃ´ng tin vÄƒn báº£n + tÃ´-pÃ´ vÃ o LLMs?

### Hypotheses (Implicit)
- **H1**: RAG â€œngÃ¢y thÆ¡â€ (truy há»“i tá»«ng tÃ i liá»‡u riÃªng láº») lÃ  khÃ´ng Ä‘á»§ cho tÃ i liá»‡u dáº¡ng máº¡ng lÆ°á»›i
- **H2**: Káº¿t há»£p tÃ´-pÃ´ Ä‘á»“ thá»‹ trong Cáº¢ truy há»“i vÃ  sinh giÃºp cáº£i thiá»‡n hiá»‡u nÄƒng LLM
- **H3**: LLM Ä‘Ã³ng bÄƒng + GRAG cÃ³ thá»ƒ vÆ°á»£t LLM Ä‘Æ°á»£c fine-tune (hiá»‡u quáº£ tham sá»‘)

**Káº¿t quáº£**:
- H1: âœ“ ÄÆ°á»£c á»§ng há»™ bá»Ÿi káº¿t quáº£ so sÃ¡nh (GRAG > RAG chá»‰ dá»±a trÃªn vÄƒn báº£n)
- H2: âœ“ ÄÆ°á»£c á»§ng há»™ bá»Ÿi ablations (graph encoder + soft pruning lÃ  then chá»‘t)
- H3: âš  Máº«u hÃ¬nh quan sÃ¡t tháº¥y nhÆ°ng KHÃ”NG cÃ³ xÃ¡c nháº­n thá»‘ng kÃª

**ÄÃ¡nh giÃ¡ phÃª bÃ¬nh**:
- âŒ Giáº£ thuyáº¿t khÃ´ng Ä‘Æ°á»£c nÃªu chÃ­nh thá»©c dÆ°á»›i dáº¡ng dá»± Ä‘oÃ¡n cÃ³ thá»ƒ kiá»ƒm Ä‘á»‹nh
- âŒ KhÃ´ng cÃ³ giáº£ thuyáº¿t cÃ³ hÆ°á»›ng kÃ¨m dá»± Ä‘oÃ¡n kÃ­ch thÆ°á»›c hiá»‡u á»©ng
- âš  Giáº£ Ä‘á»‹nh tÃ´-pÃ´ há»¯u Ã­ch nhÆ°ng khÃ´ng giáº£i thÃ­ch Táº I SAO (cÆ¡ cháº¿ chÆ°a rÃµ)

---

## Theoretical Framework

### Foundation
**RAG (Retrieval-Augmented Generation)**:
- Truyá»n thá»‘ng: Truy há»“i cÃ¡c tÃ i liá»‡u riÃªng láº» theo Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng vÄƒn báº£n â†’ Ä‘Æ°a vÃ o LLM
- Háº¡n cháº¿: Bá» qua cáº¥u trÃºc máº¡ng lÆ°á»›i thÆ°á»ng gáº·p trong dá»¯ liá»‡u thá»±c táº¿
- Tham kháº£o: Lewis et al. (2020), Guu et al. (2020), Ram et al. (2023)

### Novel Contribution
**GRAG (Graph RAG)**:
- Má»Ÿ rá»™ng RAG sang Ä‘á»“ thá»‹ vÄƒn báº£n (node/edge cÃ³ thuá»™c tÃ­nh vÄƒn báº£n)
- Quy trÃ¬nh hai giai Ä‘oáº¡n:
  1. **Retrieval**: TÃ¬m tiá»ƒu Ä‘á»“ thá»‹ vÄƒn báº£n tá»‘i Æ°u (khÃ´ng chá»‰ lÃ  tÃ i liá»‡u)
  2. **Generation**: Gá»£i lá»‡nh hai gÃ³c nhÃ¬n (vÄƒn báº£n + tÃ´-pÃ´)

### Key Assumptions
1. **Tiá»ƒu Ä‘á»“ thá»‹ quan trá»ng = há»£p cá»§a lÃ¢n cáº­n cÃ¡c node quan trá»ng**
   - âš  KhÃ´ng Ä‘Æ°á»£c biá»‡n minh vá» máº·t lÃ½ thuyáº¿t, chá»‰ Ä‘Æ°á»£c giáº£ Ä‘á»‹nh
2. **Xáº¿p háº¡ng ego-graph xáº¥p xá»‰ tiá»ƒu Ä‘á»“ thá»‹ tá»‘i Æ°u**
   - âš  Cháº¥t lÆ°á»£ng xáº¥p xá»‰ khÃ´ng bao giá» Ä‘Æ°á»£c chá»©ng minh (khÃ´ng cÃ³ cáº­n)
3. **Hai gÃ³c nhÃ¬n (vÄƒn báº£n + Ä‘á»“ thá»‹) bá»• trá»£ láº«n nhau**
   - âœ“ ÄÆ°á»£c á»§ng há»™ bá»Ÿi ablations (cáº£ hai Ä‘á»u cáº§n thiáº¿t)

**ÄÃ¡nh giÃ¡**: â­â­â­â˜†â˜† (3/5)
- Ná»n táº£ng vá»¯ng trÃªn vÄƒn liá»‡u RAG vÃ  suy luáº­n Ä‘á»“ thá»‹
- Äáº·t váº¥n Ä‘á» má»›i láº¡
- NhÆ°ng cÃ¡c biá»‡n minh lÃ½ thuyáº¿t cho cÃ¡c xáº¥p xá»‰ cÃ²n yáº¿u

---

## Methodology Overview

### Study Design
- **Type**: Thá»±c nghiá»‡m - Ä‘Ã¡nh giÃ¡ so sÃ¡nh
- **Benchmarks**: CÃ¡c bá»™ dá»¯ liá»‡u GraphQA (suy luáº­n Ä‘a bÆ°á»›c dá»±a trÃªn Ä‘á»“ thá»‹)
  - WebQSP: 4,700 Ä‘á»“ thá»‹, trung bÃ¬nh 1,370 nodes (há»i-Ä‘Ã¡p trÃªn knowledge graph)
  - ExplaGraphs: 2,766 Ä‘á»“ thá»‹, trung bÃ¬nh 5 nodes (suy luáº­n commonsense)
- **Comparisons**:
  - **Baselines**: LLM Ä‘Ã³ng bÄƒng, LLM fine-tune (LoRA)
  - **RAG methods**: BM25, MiniLM, LaBSE, mContriever, E5, G-Retriever
  - **Ablations**: 4 biáº¿n thá»ƒ (khÃ´ng truy há»“i, khÃ´ng graph encoder, khÃ´ng soft pruning, khÃ´ng vÄƒn báº£n)
- **Metrics**: F1, Hit@1, Recall (WebQSP); Accuracy (ExplaGraphs)

### Methodological Quality: â­â­â­â˜†â˜† (3/5)

**Äiá»ƒm máº¡nh**:
- Nhiá»u baseline vÃ  phÆ°Æ¡ng phÃ¡p so sÃ¡nh
- Ablation studies cÃ´ láº­p Ä‘Ã³ng gÃ³p cá»§a tá»«ng thÃ nh pháº§n
- Thá»­ nghiá»‡m transfer learning xuyÃªn bá»™ dá»¯ liá»‡u
- CÃ³ mÃ£ nguá»“n (GitHub)

**Äiá»ƒm yáº¿u**:
- âŒ KHÃ”NG cÃ³ kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a thá»‘ng kÃª (lá»—i nghiÃªm trá»ng)
- âŒ KHÃ”NG cÃ³ phÃ¢n tÃ­ch cÃ´ng suáº¥t (power analysis) hay biá»‡n minh cá»¡ máº«u
- âŒ Káº¿t quáº£ má»™t láº§n cháº¡y (khÃ´ng cÃ³ thanh sai sá»‘ ngoáº¡i trá»« Figure 3)
- âŒ KhÃ´ng cÃ³ hiá»‡u chá»‰nh so sÃ¡nh nhiá»u láº§n (48+ so sÃ¡nh)
- âš  Chá»‰ dÃ¹ng 2 bá»™ dá»¯ liá»‡u trong cÃ¹ng miá»n (graph QA)
- âš  Äáº·c tÃ­nh bá»™ dá»¯ liá»‡u ráº¥t khÃ¡c (5 vs 1,370 nodes) - nhiá»…u?

---

## GRAG Approach (Technical Summary)

### Stage 1: Textual Subgraph Retrieval

**BÃ i toÃ¡n**: TÃ¬m tiá»ƒu Ä‘á»“ thá»‹ tá»‘i Æ°u Ä tá»‘i Ä‘a hÃ³a cháº¥t lÆ°á»£ng sinh f(Ä)
- KhÃ´ng gian tÃ¬m kiáº¿m: 2^|V|+|E| (NP-hard)

**Giáº£i phÃ¡p**: Xáº¥p xá»‰ chia-Ä‘á»ƒ-trá»‹
1. **Pre-indexing** (offline):
   - MÃ£ hÃ³a má»i K-hop ego-graphs â†’ embeddings z_g
   - PLM: SentenceBERT (mean pooling trÃªn vÄƒn báº£n node/edge)
2. **Ranking** (online):
   - MÃ£ hÃ³a truy váº¥n q â†’ z_q
   - Xáº¿p háº¡ng theo cosine similarity: Top-N cos(z_q, z_g)
3. **Soft Pruning**:
   - Há»c trá»ng sá»‘ liÃªn quan: Î±_n = MLP_Ï†1(z_n âŠ– z_q)
   - Mask thÃ­ch á»©ng cÃ¡c node/edge khÃ´ng liÃªn quan trong cÃ¡c ego-graphs Ä‘Æ°á»£c chá»n

**Äá»™ phá»©c táº¡p**: O(|V|) thay vÃ¬ O(2^|V|+|E|)

**Váº¥n Ä‘á» phÃª bÃ¬nh**:
- âŒ f(Â·) â€œhÃ m cháº¥t lÆ°á»£ng sinhâ€ KHÃ”NG BAO GIá»œ Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a
- âŒ Xáº¥p xá»‰: max f(â‹ƒ subgraphs) â‰ˆ max âˆ‘ f(subgraphs) - KHÃ”NG CÃ“ CHá»¨NG MINH
- âš  ToÃ¡n tá»­ âŠ– khÃ´ng Ä‘Æ°á»£c nÃªu rÃµ (L1? L2? Cosine distance?)

### Stage 2: Textual Graph Augmented Generation

**Kiáº¿n trÃºc hai gÃ³c nhÃ¬n**:

1. **Text View (Hard Prompts)**:
   - Chuyá»ƒn ego-graph sang vÄƒn báº£n phÃ¢n cáº¥p báº±ng BFS traversal
   - Cáº¥u trÃºc cÃ¢y ğ’¯_g + cÃ¡c cáº¡nh bá»• sung â„°_g
   - Pre-order traversal â†’ mÃ´ táº£ vÄƒn báº£n thá»¥t dÃ²ng D_g
   - Báº£o toÃ n tÃ´-pÃ´ thÃ´ng qua cáº¥u trÃºc phÃ¢n cáº¥p

2. **Graph View (Soft Prompts)**:
   - MÃ£ hÃ³a báº±ng GNN (GAT: 4 layers, 4 heads, 1024 hidden)
   - Soft pruning Ä‘Æ°á»£c tÃ­ch há»£p trong message passing
   - Chiáº¿u sang khÃ´ng gian LLM: h_Ä = MLP_Ï†3(GNN_Î¦(Ä))

3. **Generation**:
   - GhÃ©p: [h_Ä (soft); h_T (hard)] â†’ LLM_Î¸
   - LLM: Llama2-7b (Ä‘Ã³ng bÄƒng hoáº·c LoRA-tuned)
   - LoRA: rank=8, Î±=16, dropout=0.05
   - Training: AdamW, lr=1e-5, 10 epochs, batch=2

**Äiá»ƒm má»›i**: Hai gÃ³c nhÃ¬n bá»• trá»£ giÃºp báº£o toÃ n cáº£ â€œmáº¡ch truyá»‡nâ€ (vÄƒn báº£n) vÃ  tÃ´-pÃ´ (Ä‘á»“ thá»‹)

---

## Key Findings Summary

### Main Results

| PhÃ¡t hiá»‡n | Báº±ng chá»©ng | Hiá»‡u á»©ng | Há»— trá»£ thá»‘ng kÃª |
|---------|----------|--------|---------------------|
| **GRAG > fine-tuned LLM** (frozen) | WebQSP Hit@1: 0.7236 vs 0.6186 | +17.0% | âŒ NO |
| **GRAG > G-Retriever** (best baseline) | WebQSP: 0.7236 vs 0.6808 | +6.3% | âŒ NO |
| **GRAG > G-Retriever** | ExplaGraphs Acc: 0.9223 vs 0.8825 | +4.5% | âŒ NO |
| **Soft pruning critical** | Ablation: 0.7236 â†’ 0.5671 w/o | -21.6% | âŒ NO |
| **Text attributes essential** | Ablation: 0.7236 â†’ 0.4496 w/o | -37.9% | âŒ NO |
| **Transfer learning** | Train WebQSPâ†’Test ExplaGraphs | +33.8% | âŒ NO |
| **Hallucination reduction** | 79% valid entities vs 62-71% | N=100 | âš  Manual eval, no IAA |

### Result Patterns

**TÃ­nh nháº¥t quÃ¡n**:
- âœ“ GRAG vÆ°á»£t táº¥t cáº£ baseline trÃªn cáº£ hai bá»™ dá»¯ liá»‡u
- âœ“ Má»i ablations Ä‘á»u lÃ m hiá»‡u nÄƒng giáº£m (thÃ nh pháº§n lÃ  cáº§n thiáº¿t)
- âœ“ Transfer learning cho tháº¥y kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a

**Äá»™ lá»›n hiá»‡u á»©ng**:
- Äá»“ thá»‹ lá»›n (WebQSP): TÄƒng vá»«a pháº£i (+6-17%)
- Äá»“ thá»‹ nhá» (ExplaGraphs): TÄƒng lá»›n hÆ¡n (+4-172%)
- âš  Gá»£i Ã½ phÆ°Æ¡ng phÃ¡p cÃ³ lá»£i hÆ¡n vá»›i Ä‘á»“ thá»‹ nhá», cÃ³ cáº¥u trÃºc

**Äá»™ vá»¯ng**:
- âš  KhÃ´ng bÃ¡o cÃ¡o tÃ¡i láº­p qua cÃ¡c random seeds
- âš  KhÃ´ng cÃ³ khoáº£ng tin cáº­y hay sai sá»‘ chuáº©n
- âš  Káº¿t quáº£ má»™t láº§n cháº¡y dá»… bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi ngáº«u nhiÃªn

---

## Evidence Quality Assessment

### Overall: â­â­â˜†â˜†â˜† (2/5) - Yáº¾U

**Nhá»¯ng gÃ¬ cÃ³**:
- âœ“ Máº«u hÃ¬nh nháº¥t quÃ¡n trÃªn cÃ¡c bá»™ dá»¯ liá»‡u vÃ  thÆ°á»›c Ä‘o
- âœ“ Nhiá»u baseline giÃºp so sÃ¡nh máº¡nh hÆ¡n
- âœ“ Ablations cho tháº¥y tÃ­nh cáº§n thiáº¿t cá»§a thÃ nh pháº§n
- âœ“ ÄÃ¡nh giÃ¡ thá»§ cÃ´ng (N=100) cho áº£o giÃ¡c

**Thiáº¿u há»¥t then chá»‘t**:
- âŒ **KHÃ”NG cÃ³ kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a thá»‘ng kÃª** (t-test, Wilcoxon, v.v.)
- âŒ **KHÃ”NG cÃ³ khoáº£ng tin cáº­y** hay lÆ°á»£ng hÃ³a báº¥t Ä‘á»‹nh
- âŒ **KHÃ”NG cÃ³ kÃ­ch thÆ°á»›c hiá»‡u á»©ng** (Cohen's d, rÂ², Î·Â²)
- âŒ **KHÃ”NG cÃ³ power analysis** - khÃ´ng rÃµ cá»¡ máº«u cÃ³ Ä‘á»§ khÃ´ng
- âŒ **KHÃ”NG hiá»‡u chá»‰nh so sÃ¡nh nhiá»u láº§n** (phá»“ng sai láº§m loáº¡i I)
- âŒ **KHÃ”NG cÃ³ chi tiáº¿t tÃ¡i láº­p** (random seeds, phÆ°Æ¡ng sai)

**Há»‡ quáº£**:
- KhÃ´ng thá»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c khÃ¡c biá»‡t cÃ³ Ä‘Ã¡ng tin vá» máº·t thá»‘ng kÃª hay chá»‰ do ngáº«u nhiÃªn
- CÃ¡c tuyÃªn bá»‘ â€œsignificantly outperformsâ€ lÃ  **khÃ´ng chÃ­nh xÃ¡c vá» máº·t ká»¹ thuáº­t** náº¿u khÃ´ng cÃ³ kiá»ƒm Ä‘á»‹nh
- Káº¿t quáº£ cÃ³ thá»ƒ khÃ´ng láº·p láº¡i (Ä‘áº·c biá»‡t vá»›i khÃ¡c biá»‡t nhá» nhÆ° +6%)

---

## Strengths

### Methodological â­â­â­â­â˜†
1. **Äá»‹nh hÃ¬nh váº¥n Ä‘á» má»›i**: Äáº§u tiÃªn giáº£i quyáº¿t rÃµ rÃ ng truy há»“i Ä‘á»“ thá»‹ vÄƒn báº£n cho RAG
2. **So sÃ¡nh toÃ n diá»‡n**: 6 retrievers + 2 LLM baselines
3. **Ablation studies**: CÃ´ láº­p há»‡ thá»‘ng tá»«ng thÃ nh pháº§n (4 biáº¿n thá»ƒ)
4. **Transfer xuyÃªn bá»™ dá»¯ liá»‡u**: Kiá»ƒm tra tá»•ng quÃ¡t hÃ³a
5. **CÃ³ mÃ£ nguá»“n**: GitHub há»— trá»£ tÃ¡i láº­p
6. **Táº­p trung hiá»‡u quáº£**: Äá»™ phá»©c táº¡p tuyáº¿n tÃ­nh qua pre-indexing

### Theoretical â­â­â­â˜†â˜†
1. **Äá»™ng cÆ¡ rÃµ**: Khoáº£ng trá»‘ng trong RAG hiá»‡n cÃ³ (bá» qua tÃ´-pÃ´)
2. **CÃ¡ch tiáº¿p cáº­n cÃ³ ná»n táº£ng**: Dá»±a trÃªn vÄƒn liá»‡u RAG vÃ  prompt tuning Ä‘Ã£ cÃ³
3. **Trá»±c giÃ¡c hai gÃ³c nhÃ¬n**: Káº¿t há»£p vÄƒn báº£n + Ä‘á»“ thá»‹ lÃ  há»£p lÃ½
4. **LiÃªn quan thá»±c tiá»…n**: Nháº¯m cÃ¡c ká»‹ch báº£n thá»±c (citations, social media, KGs)

### Technical â­â­â­â­â˜†
1. **Kiáº¿n trÃºc sÃ¡ng táº¡o**: Soft pruning + gá»£i lá»‡nh hai gÃ³c nhÃ¬n lÃ  má»›i
2. **ÄÃ³ng gÃ³p thuáº­t toÃ¡n**: Thuáº­t toÃ¡n chuyá»ƒn Ä‘á»•i vÄƒn báº£n phÃ¢n cáº¥p
3. **Xáº¥p xá»‰ hiá»‡u quáº£**: TrÃ¡nh tÃ¬m kiáº¿m lÅ©y thá»«a (dÃ¹ cháº¥t lÆ°á»£ng chÆ°a chá»©ng minh)
4. **Thiáº¿t káº¿ mÃ´-Ä‘un**: CÃ¡c thÃ nh pháº§n cÃ³ thá»ƒ nghiÃªn cá»©u Ä‘á»™c láº­p

---

## Limitations & Validity Threats

### Internal Validity ğŸŸ¡ MODERATE

**Nhiá»…u**:
1. âŒ **Äáº·c tÃ­nh bá»™ dá»¯ liá»‡u**: WebQSP (1,370 nodes) vs ExplaGraphs (5 nodes)
   - PhÆ°Æ¡ng phÃ¡p cÃ³ thá»ƒ hoáº¡t Ä‘á»™ng khÃ¡c nhau theo kÃ­ch thÆ°á»›c Ä‘á»“ thá»‹
   - Káº¿t quáº£ cho tháº¥y GRAG lá»£i hÆ¡n trÃªn Ä‘á»“ thá»‹ nhá» (nhiá»…u theo bá»™ dá»¯ liá»‡u?)
2. âš  **Tá»‘i Æ°u siÃªu tham sá»‘**: KhÃ´ng Ä‘á» cáº­p viá»‡c tá»‘i Æ°u baseline
   - CÃ¡c phÆ°Æ¡ng phÃ¡p so sÃ¡nh cÃ³ Ä‘Æ°á»£c tune cÃ´ng báº±ng khÃ´ng?
3. âš  **Lá»±a chá»n mÃ´ hÃ¬nh**: Chá»‰ thá»­ Llama2-7b
   - CÃ²n GPT-4, Claude, Gemini, Llama3?
4. âš  **Prompt engineering**: CÃ¡c phÆ°Æ¡ng phÃ¡p so sÃ¡nh cÃ³ thá»ƒ chÆ°a dÃ¹ng prompt tá»‘i Æ°u

**ÄÃ¡nh giÃ¡**: CÃ³ nhiá»…u nhÆ°ng khÃ´ng chÃ­ tá»­ - cÃ¡c káº¿t luáº­n chÃ­nh cÃ³ thá»ƒ váº«n Ä‘Ãºng

### External Validity ğŸ”´ CRITICAL

**Kháº£ nÄƒng khÃ¡i quÃ¡t quáº§n thá»ƒ**:
1. âŒ **Miá»n háº¡n cháº¿**: Chá»‰ knowledge graphs + commonsense reasoning
   - Äá»™ng cÆ¡ ban Ä‘áº§u gá»“m: citations, social media, product reviews
   - **KHÃ”NG cÃ¡i nÃ o Ä‘Æ°á»£c kiá»ƒm tra!**
2. âŒ **Loáº¡i Ä‘á»“ thá»‹**: Äá»“ thá»‹ thÆ°a, sáº¡ch, cÃ³ cáº¥u trÃºc
   - Thá»±c táº¿: dÃ y, nhiá»…u, thiáº¿u, Ä‘á»™ng
3. âŒ **Loáº¡i tÃ¡c vá»¥**: Chá»‰ multi-hop QA
   - CÃ²n: tÃ³m táº¯t, há»™i thoáº¡i, gá»£i Ã½, giáº£i thÃ­ch?
4. âŒ **NgÃ´n ngá»¯**: Chá»‰ tiáº¿ng Anh
   - Kháº£ nÄƒng khÃ¡i quÃ¡t Ä‘a ngÃ´n ngá»¯ chÆ°a rÃµ

**Äá»™ giÃ¡ trá»‹ sinh thÃ¡i**:
- Bá»™ benchmark â‰  triá»ƒn khai thá»±c táº¿
- KhÃ´ng cÃ³ nghiÃªn cá»©u ngÆ°á»i dÃ¹ng hay A/B tests
- Thiáº¿u: yÃªu cáº§u Ä‘á»™ trá»…, rÃ ng buá»™c chi phÃ­, sá»Ÿ thÃ­ch con ngÆ°á»i

**Má»©c Ä‘á»™**: ğŸ”´ **CRITICAL** - háº¡n cháº¿ nghiÃªm trá»ng cÃ¡c tuyÃªn bá»‘ vá» tÃ­nh á»©ng dá»¥ng thá»±c táº¿

### Statistical Conclusion Validity ğŸ”´ CRITICAL

**Váº¥n Ä‘á» lá»›n**:
1. âŒ **KHÃ”NG kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a**: KhÃ´ng thá»ƒ biáº¿t káº¿t quáº£ cÃ³ Ä‘Ã¡ng tin khÃ´ng
2. âŒ **KHÃ”NG khoáº£ng tin cáº­y**: KhÃ´ng lÆ°á»£ng hÃ³a báº¥t Ä‘á»‹nh
3. âŒ **KHÃ”NG power analysis**: KhÃ´ng rÃµ cá»¡ máº«u cÃ³ Ä‘á»§ khÃ´ng
4. âŒ **So sÃ¡nh nhiá»u láº§n**: 48+ so sÃ¡nh (6 phÆ°Æ¡ng phÃ¡p Ã— 4 metrics Ã— 2 bá»™ dá»¯ liá»‡u)
   - XÃ¡c suáº¥t sai láº§m loáº¡i I â‰ˆ 1 - (1-0.05)^48 = 91%!
   - NÃªn dÃ¹ng Bonferroni (Î±=0.05/48=0.001) hoáº·c hiá»‡u chá»‰nh FDR
5. âŒ **Káº¿t quáº£ má»™t láº§n cháº¡y**: KhÃ´ng cÃ³ phÆ°Æ¡ng sai theo random seeds
6. âš  **Dá»¯ liá»‡u phi tham sá»‘**: Hit@1, Accuracy lÃ  dáº¡ng phÃ¢n loáº¡i
   - NÃªn dÃ¹ng Wilcoxon, Friedman tests (khÃ´ng Ä‘Æ°á»£c bÃ¡o cÃ¡o)

**Há»‡ quáº£**:
- **KhÃ´ng thá»ƒ tin Ä‘á»™ tin cáº­y thá»‘ng kÃª** cá»§a cÃ¡c khÃ¡c biá»‡t Ä‘Æ°á»£c bÃ¡o cÃ¡o
- Cáº£i thiá»‡n nhá» (+6%) ráº¥t dá»… bá»‹ dao Ä‘á»™ng ngáº«u nhiÃªn
- CÃ¡c tuyÃªn bá»‘ â€œsignificantâ€ vÆ°á»£t trá»™i lÃ  **gÃ¢y hiá»ƒu láº§m**

**Má»©c Ä‘á»™**: ğŸ”´ **CRITICAL** - lÃ m suy yáº¿u toÃ n bá»™ káº¿t luáº­n Ä‘á»‹nh lÆ°á»£ng

### Construct Validity ğŸŸ¡ MODERATE

**Äo lÆ°á»ng**:
- âœ“ F1, Hit@1, Recall, Accuracy lÃ  chuáº©n cho QA
- âš  ChÃºng cÃ³ pháº£n Ã¡nh â€œhiá»ƒu tÃ i liá»‡u dáº¡ng máº¡ng lÆ°á»›iâ€ khÃ´ng?
- âš  ÄÃºng/sai nhá»‹ phÃ¢n (Hit@1) xem má»i lá»—i nhÆ° nhau
- âŒ KhÃ´ng Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i ngoÃ i Ä‘á»™ Ä‘Ãºng

**TÃ¡c Ä‘á»™ng thao tÃ¡c (manipulation)**:
- âŒ KhÃ´ng xÃ¡c minh soft pruning cÃ³ mask thá»±c sá»± cÃ¡c thá»±c thá»ƒ khÃ´ng liÃªn quan
- âŒ KhÃ´ng phÃ¢n tÃ­ch cháº¥t lÆ°á»£ng tiá»ƒu Ä‘á»“ thá»‹ truy há»“i (precision/recall)
- âš  HÃ m f(Â·) khÃ´ng Ä‘Æ°á»£c váº­n hÃ nh hÃ³a - â€œcháº¥t lÆ°á»£ng sinhâ€ lÃ  gÃ¬?

**ÄÃ¡nh giÃ¡**: Há»£p lÃ½ nhÆ°ng chÆ°a Ä‘áº§y Ä‘á»§ - thiáº¿u cÃ¡c kiá»ƒm Ä‘á»‹nh cáº¥u trÃºc quan trá»ng

---

## Critical Evaluation

### What Paper Does Exceptionally Well

1. **Nháº­n diá»‡n váº¥n Ä‘á»**: NÃªu rÃµ khoáº£ng trá»‘ng (RAG bá» qua tÃ´-pÃ´)
2. **Äá»•i má»›i ká»¹ thuáº­t**: Kiáº¿n trÃºc hai gÃ³c nhÃ¬n thanh thoÃ¡t vÃ  má»›i
3. **Hiá»‡u quáº£**: Xáº¥p xá»‰ thá»i gian tuyáº¿n tÃ­nh lÃ  thá»±c tiá»…n
4. **TÃ­nh mÃ´-Ä‘un**: TÃ¡ch sáº¡ch retrieval vÃ  generation
5. **Minh báº¡ch**: MÃ£ nguá»“n cÃ³ sáºµn, mÃ´ táº£ kiáº¿n trÃºc chi tiáº¿t

### What Could Be Improved

#### Methodology
1. **Äá»™ nghiÃªm ngáº·t thá»‘ng kÃª**: ThÃªm kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a, khoáº£ng tin cáº­y, kÃ­ch thÆ°á»›c hiá»‡u á»©ng
2. **TÃ¡i láº­p**: Nhiá»u random seeds, bÃ¡o cÃ¡o phÆ°Æ¡ng sai
3. **Power analysis**: Biá»‡n minh cá»¡ máº«u, Ä‘áº£m báº£o power (0.80)
4. **Tá»•ng quÃ¡t hÃ³a**: Thá»­ trÃªn citation graphs, social media, cÃ¡c miá»n khÃ¡c
5. **Baselines**: ThÃªm baseline â€œlong contextâ€ (toÃ n bá»™ Ä‘á»“ thá»‹ thÃ nh vÄƒn báº£n pháº³ng)

#### Analysis
1. **PhÃ¢n tÃ­ch lá»—i**: NghiÃªn cá»©u Ä‘á»‹nh tÃ­nh cÃ¡c trÆ°á»ng há»£p tháº¥t báº¡i
2. **Cháº¥t lÆ°á»£ng tiá»ƒu Ä‘á»“ thá»‹**: PhÃ¢n tÃ­ch precision/recall cá»§a node/edge Ä‘Æ°á»£c truy há»“i
3. **Chi phÃ­ tÃ­nh toÃ¡n**: BÃ¡o cÃ¡o runtime, bá»™ nhá»›, thá»i gian huáº¥n luyá»‡n
4. **Ablations**: Thá»­ cÃ¡c giáº£i thÃ­ch thay tháº¿ (tiá»ƒu Ä‘á»“ thá»‹ ngáº«u nhiÃªn? Ä‘á»™ dÃ i prompt?)
5. **ÄÃ¡nh giÃ¡ con ngÆ°á»i**: Má»Ÿ rá»™ng sang cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i, khÃ´ng chá»‰ áº£o giÃ¡c (N=100â†’N=500, IAA)

#### Interpretation
1. **CÆ¡ cháº¿**: Giáº£i thÃ­ch Táº I SAO tÃ´-pÃ´ há»¯u Ã­ch (hiá»‡n táº¡i: chá»‰ giáº£ Ä‘á»‹nh)
2. **Xáº¥p xá»‰**: Chá»©ng minh hoáº·c kiá»ƒm chá»©ng thá»±c nghiá»‡m cháº¥t lÆ°á»£ng chia-Ä‘á»ƒ-trá»‹
3. **Háº¡n cháº¿**: Thá»«a nháº­n thiáº¿u há»¥t thá»‘ng kÃª, lo ngáº¡i tá»•ng quÃ¡t hÃ³a
4. **Ã nghÄ©a thá»±c tiá»…n**: Tháº£o luáº­n liá»‡u +6% cÃ³ Ä‘Ã¡ng vá»›i Ä‘á»™ phá»©c táº¡p

### Alternative Explanations

1. **Hiá»‡u á»©ng Ä‘á»™ dÃ i prompt**:
   - **Thay tháº¿**: Lá»£i Ã­ch Ä‘áº¿n tá»« cung cáº¥p NHIá»€U ngá»¯ cáº£nh hÆ¡n, khÃ´ng pháº£i cáº¥u trÃºc Ä‘á»“ thá»‹
   - **Kiá»ƒm thá»­**: Khá»‘ng cháº¿ tá»•ng sá»‘ token giá»¯a cÃ¡c phÆ°Æ¡ng phÃ¡p
   - **Kháº£ dÄ©**: CAO - prompt dÃ i thÆ°á»ng giÃºp LLMs
   - **Pháº£n biá»‡n cá»§a tÃ¡c giáº£**: Ablation khÃ´ng graph encoder váº«n cÃ³ vÄƒn báº£n phÃ¢n cáº¥p

2. **Máº«u hÃ¬nh Ä‘áº·c thÃ¹ bá»™ dá»¯ liá»‡u**:
   - **Thay tháº¿**: GRAG overfit vÃ o Ä‘áº·c tÃ­nh cá»§a benchmark GraphQA
   - **Kiá»ƒm thá»­**: ÄÃ¡nh giÃ¡ trÃªn tÃ¡c vá»¥ out-of-domain (khÃ´ng suy luáº­n Ä‘á»“ thá»‹)
   - **Kháº£ dÄ©**: TRUNG BÃŒNH - cáº£ hai bá»™ dá»¯ liá»‡u Ä‘á»u Ä‘Æ°á»£c thiáº¿t káº¿ cho suy luáº­n Ä‘á»“ thá»‹

3. **Fine-tuning chÆ°a tá»‘i Æ°u**:
   - **Thay tháº¿**: Baseline fine-tuning cáº¥u hÃ¬nh kÃ©m, khÃ´ng pháº£i retrieval > tuning
   - **Kiá»ƒm thá»­**: TÃ¬m kiáº¿m siÃªu tham sá»‘ cho cÃ¡c baseline LoRA
   - **Kháº£ dÄ©**: TRUNG BÃŒNH - chá»‰ 10 epochs, má»™t learning rate

4. **Biáº¿n thiÃªn ngáº«u nhiÃªn**:
   - **Thay tháº¿**: KhÃ¡c biá»‡t do â€œmay máº¯nâ€ random seed
   - **Kiá»ƒm thá»­**: Kiá»ƒm Ä‘á»‹nh thá»‘ng kÃª, nhiá»u láº§n cháº¡y
   - **Kháº£ dÄ©**: CAO cho khÃ¡c biá»‡t nhá» (+6%) náº¿u khÃ´ng cÃ³ kiá»ƒm Ä‘á»‹nh

### Unanswered Questions

1. **CÆ¡ cháº¿**: TÃ´-pÃ´ giÃºp nhÆ° tháº¿ nÃ o? Do cáº¥u trÃºc ego-graph hay do biá»ƒu diá»…n GNN?
2. **Cháº¥t lÆ°á»£ng xáº¥p xá»‰**: Chia-Ä‘á»ƒ-trá»‹ cÃ¡ch xa tiá»ƒu Ä‘á»“ thá»‹ tá»‘i Æ°u bao nhiÃªu?
3. **Kháº£ nÄƒng má»Ÿ rá»™ng**: Äiá»u gÃ¬ xáº£y ra vá»›i Ä‘á»“ thá»‹ triá»‡u node (Wikipedia, Twitter)?
4. **Äá»“ thá»‹ Ä‘á»™ng**: Xá»­ lÃ½ cáº­p nháº­t thÆ°á»ng xuyÃªn tháº¿ nÃ o (social media, news)?
5. **Äá»“ thá»‹ khÃ´ng hoÃ n chá»‰nh**: Hiá»‡u nÄƒng khi thiáº¿u cáº¡nh, thuá»™c tÃ­nh nhiá»…u?
6. **Chi phÃ­ thá»±c tiá»…n**: +6% cÃ³ Ä‘Ã¡ng vá»›i Ä‘á»™ phá»©c táº¡p vÃ  Ä‘á»™ trá»… tÄƒng thÃªm?
7. **XuyÃªn miá»n**: GRAG cÃ³ giÃºp cÃ¡c tÃ¡c vá»¥ khÃ´ng pháº£i QA khÃ´ng? TÃ i liá»‡u khÃ´ng dáº¡ng Ä‘á»“ thá»‹?

---

## Reproducibility Assessment

### Checklist

- [x] PhÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c mÃ´ táº£ Ä‘á»§ chi tiáº¿t (pháº§n lá»›n)
- [ ] Cá»¡ máº«u Ä‘Æ°á»£c biá»‡n minh (power analysis) âŒ NO
- [x] Dá»¯ liá»‡u sáºµn cÃ³ (cÃ¡c benchmark hiá»‡n há»¯u)
- [x] Code available (GitHub: https://github.com/HuieL/GRAG)
- [ ] Random seeds Ä‘Æ°á»£c bÃ¡o cÃ¡o âŒ NO
- [ ] TÃ¬m kiáº¿m siÃªu tham sá»‘ Ä‘Æ°á»£c ghi nháº­n âš  Minimal
- [ ] YÃªu cáº§u compute âš  ThÃ´ng sá»‘ GPU cÃ³, nhÆ°ng khÃ´ng cÃ³ runtime/cost
- [ ] Pre-registered âŒ N/A (khÃ´ng ká»³ vá»ng)
- [ ] Chi tiáº¿t kiá»ƒm Ä‘á»‹nh thá»‘ng kÃª âŒ NONE conducted
- [ ] Tá»‘i Æ°u baseline âš  Unclear

### Score: â­â­â­â˜†â˜† (3/5) - MEDIUM

**CÃ³ thá»ƒ tÃ¡i láº­p**:
- âœ“ CÃ¡ch tiáº¿p cáº­n tá»•ng quÃ¡t (code cÃ³ sáºµn)
- âœ“ Chi tiáº¿t kiáº¿n trÃºc (Ä‘Æ°á»£c Ä‘áº·c táº£ tá»‘t)
- âœ“ Quy trÃ¬nh huáº¥n luyá»‡n (siÃªu tham sá»‘ Ä‘Æ°á»£c Ä‘Æ°a ra)

**KhÃ´ng thá»ƒ tÃ¡i láº­p**:
- âŒ Con sá»‘ chÃ­nh xÃ¡c (khÃ´ng cÃ³ random seeds)
- âŒ Cáº¥u hÃ¬nh baseline (tá»‘i Æ°u khÃ´ng rÃµ)
- âš  PhiÃªn báº£n PLM (SentenceBERT - phiÃªn báº£n nÃ o?)

**Kháº£ nÄƒng tÃ¡i láº­p**: **MEDIUM**
- CÃ³ thá»ƒ tÃ¡i láº­p xu hÆ°á»›ng chung (GRAG > baselines)
- CÃ¡c metric chÃ­nh xÃ¡c cÃ³ thá»ƒ lá»‡ch Â±5% náº¿u khÃ´ng cÃ³ seeds
- Äá»™ tin cáº­y thá»‘ng kÃª khÃ´ng cháº¯c náº¿u khÃ´ng cÃ³ kiá»ƒm Ä‘á»‹nh

---

## Contribution to Field

### Theoretical Contribution â­â­â­â­â˜†

**Tiáº¿n bá»™**:
1. **BÃ i toÃ¡n má»›i**: Truy há»“i Ä‘á»“ thá»‹ vÄƒn báº£n cho RAG (Ä‘áº·t váº¥n Ä‘á» Ä‘áº§u tiÃªn)
2. **Khung lÃ m viá»‡c**: GRAG má»Ÿ rá»™ng lÃ½ thuyáº¿t RAG sang tÃ i liá»‡u dáº¡ng máº¡ng lÆ°á»›i
3. **Nháº­n Ä‘á»‹nh**: TÃ­nh bá»• trá»£ cá»§a hai gÃ³c nhÃ¬n (vÄƒn báº£n + tÃ´-pÃ´)

**Háº¡n cháº¿**:
- LÃ½ thuyáº¿t xáº¥p xá»‰ chÆ°a hoÃ n chá»‰nh (khÃ´ng cÃ³ cáº­n)
- CÆ¡ cháº¿ khÃ´ng Ä‘Æ°á»£c giáº£i thÃ­ch (táº¡i sao tÃ´-pÃ´ giÃºp)

### Methodological Contribution â­â­â­â­â˜†

**Tiáº¿n bá»™**:
1. **Thuáº­t toÃ¡n**: Chuyá»ƒn Ä‘á»•i vÄƒn báº£n phÃ¢n cáº¥p cho ego-graphs
2. **Kiáº¿n trÃºc**: Soft pruning + gá»£i lá»‡nh hai gÃ³c nhÃ¬n
3. **Hiá»‡u quáº£**: Truy há»“i tiá»ƒu Ä‘á»“ thá»‹ thá»i gian tuyáº¿n tÃ­nh

**Háº¡n cháº¿**:
- Thiáº¿u phÆ°Æ¡ng phÃ¡p thá»‘ng kÃª (khoáº£ng trá»‘ng nghiÃªm trá»ng)
- ÄÃ¡nh giÃ¡ giá»›i háº¡n á»Ÿ graph QA

### Empirical Contribution â­â­â­â˜†â˜†

**Tiáº¿n bá»™**:
1. **Báº±ng chá»©ng**: Ngá»¯ cáº£nh Ä‘á»“ thá»‹ giÃºp LLMs (máº«u hÃ¬nh nháº¥t quÃ¡n)
2. **PhÃ¡t hiá»‡n**: LLM Ä‘Ã³ng bÄƒng + GRAG cáº¡nh tranh vá»›i fine-tuning
3. **Chuyá»ƒn giao**: Tá»•ng quÃ¡t hÃ³a xuyÃªn bá»™ dá»¯ liá»‡u Ä‘Æ°á»£c minh há»a

**Háº¡n cháº¿**:
- Äá»™ tin cáº­y thá»‘ng kÃª chÆ°a rÃµ
- Giá»›i háº¡n á»Ÿ 2 bá»™ dá»¯ liá»‡u tÆ°Æ¡ng tá»±
- Thiáº¿u xÃ¡c thá»±c thá»±c táº¿

### Practical Implications â­â­â­â˜†â˜†

**Cho ngÆ°á»i lÃ m thá»±c (Practitioners)**:
1. âœ“ Khi lÃ m viá»‡c vá»›i tÃ i liá»‡u dáº¡ng máº¡ng lÆ°á»›i, hÃ£y cÃ¢n nháº¯c cáº¥u trÃºc Ä‘á»“ thá»‹
2. âœ“ Gá»£i lá»‡nh hai gÃ³c nhÃ¬n cÃ³ thá»ƒ cáº£i thiá»‡n suy luáº­n Ä‘á»“ thá»‹
3. âš  ChÆ°a rÃµ lá»£i Ã­ch/chi phÃ­ (+6% cÃ³ Ä‘Ã¡ng vá»›i Ä‘á»™ phá»©c táº¡p tÄƒng thÃªm?)

**Cho nhÃ  nghiÃªn cá»©u**:
1. âœ“ Má»Ÿ ra hÆ°á»›ng nghiÃªn cá»©u má»›i (textual graph RAG)
2. âœ“ Má»™t baseline cho cÃ¡c nghiÃªn cá»©u tÆ°Æ¡ng lai vá» LLMs tÄƒng cÆ°á»ng Ä‘á»“ thá»‹
3. âš  Cáº§n tÃ¡i láº­p vá»›i Ä‘á»™ nghiÃªm ngáº·t thá»‘ng kÃª

**Cho chÃ­nh sÃ¡ch**:
- Ãt hÃ m Ã½ trá»±c tiáº¿p cho chÃ­nh sÃ¡ch (Ä‘Ã³ng gÃ³p ká»¹ thuáº­t)

### Overall Impact: â­â­â­â­â˜† (4/5) - TIá»€M NÄ‚NG CAO

**Äiá»ƒm máº¡nh**:
- BÃ i toÃ¡n má»›i vá»›i kháº£ nÄƒng Ã¡p dá»¥ng rá»™ng (citations, social media, KGs)
- ÄÃ³ng gÃ³p ká»¹ thuáº­t Ä‘Æ°á»£c thá»±c hiá»‡n tá»‘t
- Nhiá»u kháº£ nÄƒng thÃºc Ä‘áº©y nghiÃªn cá»©u tiáº¿p theo

**Äiá»ƒm yáº¿u**:
- Khoáº£ng trá»‘ng nghiÃªm ngáº·t thá»‘ng kÃª lÃ m giáº£m Ä‘á»™ tin
- ChÆ°a biáº¿t tá»•ng quÃ¡t hÃ³a Ä‘áº¿n Ä‘Ã¢u nÃªn khÃ³ Ã¡p dá»¥ng ngay
- Cáº§n xÃ¡c thá»±c chi phÃ­-lá»£i Ã­ch cho triá»ƒn khai thá»±c táº¿

---

## Position in Literature

### Builds On

**RAG Foundation**:
- Lewis et al. (2020): RAG framework
- Guu et al. (2020): REALM (retrieval-augmented LM pre-training)
- Ram et al. (2023): In-context RAG
- Gao et al. (2023): RAG survey
- **Pháº§n má»Ÿ rá»™ng cá»§a GRAG**: ÄÆ°a tÃ´-pÃ´ Ä‘á»“ thá»‹ vÃ o retrieval + generation

**Prompt Tuning**:
- Lester et al. (2021): Soft prompt learning
- Shin et al. (2020): AutoPrompt (hard prompts)
- **ÄÃ³ng gÃ³p cá»§a GRAG**: Dual-view (hard + soft) cho Ä‘á»“ thá»‹

**LLMs on Graphs**:
- Chen et al. (2024): Graph reasoning with LLMs
- He et al. (2024): G-Retriever (baseline gáº§n nháº¥t)
- Hu et al. (2023b): LLM text embeddings for graphs
- **Äiá»ƒm má»›i cá»§a GRAG**: Truy há»“i tiá»ƒu Ä‘á»“ thá»‹ (khÃ´ng chá»‰ nodes/triples), sinh theo hai gÃ³c nhÃ¬n

**Graph Retrieval**:
- Yasunaga et al. (2021): QA-GNN (truy há»“i cÃ¡c node liÃªn quan)
- Kang et al. (2023): Triple retrieval
- Edge et al. (2024): Community-based retrieval
- **Tiáº¿n bá»™ cá»§a GRAG**: Soft pruning + xáº¥p xá»‰ ego-graph

### Related Concurrent Work

- G-Retriever (He et al., 2024): Giá»‘ng nháº¥t - truy há»“i tiá»ƒu Ä‘á»“ thá»‹, soft prompts
  - **KhÃ¡c biá»‡t**: GRAG thÃªm gÃ³c nhÃ¬n vÄƒn báº£n phÃ¢n cáº¥p + soft pruning
- Graph Prompt Tuning (Perozzi et al., 2024; Tian et al., 2024): MÃ£ hÃ³a tÃ´-pÃ´ qua prompts
  - **KhÃ¡c biá»‡t**: GRAG tÃ­ch há»£p cáº£ giai Ä‘oáº¡n retrieval

### Future Work Building on This

**HÆ°á»›ng cÃ³ kháº£ nÄƒng**:
1. **XÃ¡c thá»±c thá»‘ng kÃª**: TÃ¡i láº­p vá»›i kiá»ƒm Ä‘á»‹nh nghiÃªm ngáº·t
2. **Tá»•ng quÃ¡t hÃ³a**: Citation graphs, social networks, product reviews
3. **Má»Ÿ rá»™ng quy mÃ´**: Äá»“ thá»‹ triá»‡u node, cáº­p nháº­t Ä‘á»™ng
4. **NghiÃªn cá»©u cÆ¡ cháº¿**: Táº¡i sao tÃ´-pÃ´ giÃºp? TÃ­nh diá»…n giáº£i
5. **Hiá»‡u quáº£ chi phÃ­**: Tá»‘i Æ°u Ä‘á»™ trá»…, bá»™ nhá»›
6. **XuyÃªn miá»n**: TÃ¡c vá»¥ khÃ´ng QA (tÃ³m táº¯t, há»™i thoáº¡i)
7. **LÃ½ thuyáº¿t**: Chá»©ng minh cáº­n xáº¥p xá»‰, tÃ­nh tá»‘i Æ°u tiá»ƒu Ä‘á»“ thá»‹

---

## Future Research Directions

### Suggested by Authors (Sec.7 Limitations)

1. **Retrieval efficiency**: Cáº£i thiá»‡n xáº¿p háº¡ng ego-graph vÃ  cháº¥t lÆ°á»£ng pruning
   - Hiá»‡n táº¡i: Phá»¥ thuá»™c vÃ o xáº¿p háº¡ng node ban Ä‘áº§u
   - ThÃ¡ch thá»©c: KhÃ³ Æ°á»›c lÆ°á»£ng cáº¥u trÃºc Ä‘á»“ thá»‹

**Ghi chÃº phÃª bÃ¬nh**: TÃ¡c giáº£ chá»‰ thá»«a nháº­n Má»˜T háº¡n cháº¿ (phá»¥ thuá»™c vÃ o cháº¥t lÆ°á»£ng truy há»“i ban Ä‘áº§u). Äiá»u nÃ y lÃ  KHÃ”NG Äá»¦ - nhiá»u khoáº£ng trá»‘ng quan trá»ng khÃ´ng Ä‘Æ°á»£c nÃªu.

### Critical Next Steps (My Evaluation)

#### High Priority ğŸ”´
1. **XÃ¡c thá»±c thá»‘ng kÃª**: TÃ¡i láº­p vá»›i kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a, khoáº£ng tin cáº­y
2. **Kiá»ƒm tra tá»•ng quÃ¡t hÃ³a**: Citation graphs, social media, product reviews (vÃ­ dá»¥ Ä‘á»™ng cÆ¡!)
3. **PhÃ¢n tÃ­ch chi phÃ­-lá»£i Ã­ch**: Runtime, memory, latency vs gains vá» Ä‘á»™ Ä‘Ãºng
4. **PhÃ¢n tÃ­ch lá»—i**: NghiÃªn cá»©u Ä‘á»‹nh tÃ­nh khi nÃ o/táº¡i sao GRAG tháº¥t báº¡i
5. **Giáº£i thÃ­ch thay tháº¿**: Khá»‘ng cháº¿ Ä‘á»™ dÃ i prompt, thá»­ tiá»ƒu Ä‘á»“ thá»‹ ngáº«u nhiÃªn

#### Medium Priority ğŸŸ¡
6. **NghiÃªn cá»©u cÆ¡ cháº¿**: Táº¡i sao tÃ´-pÃ´ giÃºp? Ablations theo thuá»™c tÃ­nh Ä‘á»“ thá»‹
7. **LÃ½ thuyáº¿t xáº¥p xá»‰**: Chá»©ng minh cáº­n hoáº·c xÃ¡c thá»±c thá»±c nghiá»‡m cháº¥t lÆ°á»£ng chia-Ä‘á»ƒ-trá»‹
8. **Má»Ÿ rá»™ng quy mÃ´**: Thá»­ trÃªn Ä‘á»“ thá»‹ triá»‡u node (Wikipedia, social networks)
9. **Äá»“ thá»‹ Ä‘á»™ng**: Xá»­ lÃ½ cáº­p nháº­t thÆ°á»ng xuyÃªn (yÃªu cáº§u thá»±c táº¿)
10. **ÄÃ¡nh giÃ¡ con ngÆ°á»i**: Má»Ÿ rá»™ng sang cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i, sá»Ÿ thÃ­ch ngÆ°á»i dÃ¹ng (N=500+, IAA)

#### Low Priority ğŸŸ¢
11. **Äa ngÃ´n ngá»¯**: NgÃ´n ngá»¯ khÃ´ng pháº£i tiáº¿ng Anh
12. **Äa phÆ°Æ¡ng thá»©c**: áº¢nh/video trong cÃ¡c node cá»§a Ä‘á»“ thá»‹
13. **Há»c chá»§ Ä‘á»™ng**: Tiá»ƒu Ä‘á»“ thá»‹ nÃ o cáº§n gÃ¡n nhÃ£n Ä‘á»ƒ cáº£i thiá»‡n
14. **LiÃªn hiá»‡p (federated)**: Truy há»“i Ä‘á»“ thá»‹ báº£o vá»‡ riÃªng tÆ°

---

## Key Takeaways

### For Researchers ğŸ“š

1. **ChÃ­nh**: TÃ´-pÃ´ Ä‘á»“ thá»‹ cáº£i thiá»‡n RAG cho tÃ i liá»‡u dáº¡ng máº¡ng lÆ°á»›i (máº«u hÃ¬nh nháº¥t quÃ¡n)
2. **LÆ°u Ã½**: Äá»™ tin cáº­y thá»‘ng kÃª chÆ°a Ä‘Æ°á»£c kiá»ƒm chá»©ng - cáº§n tÃ¡i láº­p nghiÃªm ngáº·t
3. **Äá»•i má»›i**: Kiáº¿n trÃºc hai gÃ³c nhÃ¬n (vÄƒn báº£n + Ä‘á»“ thá»‹) thanh thoÃ¡t vÃ  mÃ´-Ä‘un
4. **CÃ¢u há»i má»Ÿ**: CÆ¡ cháº¿, cháº¥t lÆ°á»£ng xáº¥p xá»‰, tá»•ng quÃ¡t hÃ³a, chi phÃ­-lá»£i Ã­ch
5. **Tiáº¿p theo**: TÃ¡i láº­p vá»›i kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a, thá»­ trÃªn cÃ¡c miá»n thá»±c táº¿

### For Practitioners ğŸ’¼

1. **TÃ¬nh huá»‘ng dÃ¹ng**: Náº¿u lÃ m vá»›i citations, social networks, KGs â†’ cÃ¢n nháº¯c GRAG
2. **ÄÃ¡nh Ä‘á»•i**: +6-17% tÄƒng Ä‘á»™ Ä‘Ãºng vs Ä‘á»™ phá»©c táº¡p tÄƒng thÃªm (pre-indexing, GNN, soft pruning)
3. **Háº¡n cháº¿**: Chá»‰ Ä‘Æ°á»£c kiá»ƒm chá»©ng trÃªn benchmark graph QA, khÃ´ng pháº£i RAG tá»•ng quÃ¡t
4. **Tháº­n trá»ng**: CÃ³ thá»ƒ khÃ´ng tá»•ng quÃ¡t cho Ä‘á»“ thá»‹ dÃ y/nhiá»…u/Ä‘á»™ng
5. **Code**: CÃ³ trÃªn GitHub - cÃ³ thá»ƒ thá»­ trá»±c tiáº¿p

### For Students ğŸ“–

1. **Äáº·t váº¥n Ä‘á»**: VÃ­ dá»¥ tá»‘t vá» nháº­n diá»‡n khoáº£ng trá»‘ng (RAG bá» qua tÃ´-pÃ´)
2. **Thá»±c thi ká»¹ thuáº­t**: Kiáº¿n trÃºc Ä‘Æ°á»£c thiáº¿t káº¿ tá»‘t, trÃ¬nh bÃ y rÃµ
3. **Khoáº£ng trá»‘ng thá»‘ng kÃª**: BÃ i há»c - LUÃ”N cÃ³ kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a, CIs, kÃ­ch thÆ°á»›c hiá»‡u á»©ng
4. **TÃ¡i láº­p**: Code + chi tiáº¿t â‰  tÃ¡i láº­p hoÃ n háº£o (cáº§n seeds, cáº¥u hÃ¬nh chÃ­nh xÃ¡c)
5. **Äá»c pháº£n biá»‡n**: Äá»«ng tin â€œsignificantly outperformsâ€ náº¿u khÃ´ng cÃ³ báº±ng chá»©ng thá»‘ng kÃª

---

## Personal Research Notes

### Credibility: â­â­â­â˜†â˜† (3/5)

**CÃ³ tin cÃ¡c phÃ¡t hiá»‡n khÃ´ng?**
- âš  **Máº«u hÃ¬nh cÃ³ váº» há»£p lÃ½** (GRAG > baselines nháº¥t quÃ¡n)
- âš  **NhÆ°ng khÃ´ng cÃ³ kiá»ƒm Ä‘á»‹nh thá»‘ng kÃª**, khÃ´ng loáº¡i trá»« Ä‘Æ°á»£c ngáº«u nhiÃªn
- âš  **Cáº£i thiá»‡n khiÃªm tá»‘n** (+6% so vá»›i G-Retriever) cÃ³ thá»ƒ khÃ´ng láº·p láº¡i

**Khuyáº¿n nghá»‹ trÃ­ch dáº«n?**
- âœ“ **CÃ³ thá»ƒ cho**: Äáº·t váº¥n Ä‘á», Ã½ tÆ°á»Ÿng kiáº¿n trÃºc, cÃ¡ch tiáº¿p cáº­n ká»¹ thuáº­t
- âš  **Tháº­n trá»ng cho**: TuyÃªn bá»‘ Ä‘á»‹nh lÆ°á»£ng (kÃ¨m lÆ°u Ã½ vá» thiáº¿u kiá»ƒm Ä‘á»‹nh)
- âŒ **TrÃ¡nh**: Kháº³ng Ä‘á»‹nh â€œGRAG significantly outperformsâ€ náº¿u khÃ´ng cÃ³ tÃ¡i láº­p

**Quality Tier**: **Máº¡nh nhÆ°ng cÃ³ lá»—i**
- ÄÃ³ng gÃ³p ká»¹ thuáº­t cáº¥p cao
- NhÆ°ng thiáº¿u há»¥t thá»‘ng kÃª nghiÃªm trá»ng ngÄƒn má»©c â€œxuáº¥t sáº¯câ€

### Relevance to My Work

**Insight há»¯u Ã­ch**:
1. Gá»£i lá»‡nh hai gÃ³c nhÃ¬n (hard + soft) lÃ  má»™t máº«u hÃ¬nh máº¡nh
2. Chiáº¿n lÆ°á»£c xáº¥p xá»‰ cho bÃ i toÃ¡n Ä‘á»“ thá»‹ NP-hard
3. Táº§m quan trá»ng cá»§a soft pruning (cÆ¡ cháº¿ attention) cho truy há»“i nhiá»…u

**Má»‘i lo**:
1. Tá»•ng quÃ¡t hÃ³a ngoÃ i graph QA chÆ°a cháº¯c
2. Chi phÃ­ tÃ­nh toÃ¡n khÃ´ng Ä‘Æ°á»£c phÃ¢n tÃ­ch (cÃ³ thá»ƒ quÃ¡ cao)
3. Äá»™ tin cáº­y thá»‘ng kÃª cáº§n Ä‘Æ°á»£c kiá»ƒm chá»©ng Ä‘á»™c láº­p

### Papers to Read Next

**Foundations**:
1. Lewis et al. (2020): RAG framework - hiá»ƒu baseline
2. Lester et al. (2021): Soft prompt tuning - hiá»ƒu soft prompts
3. He et al. (2024): G-Retriever - Ä‘á»‘i thá»§ gáº§n nháº¥t

**Extensions**:
4. Edge et al. (2024): Community-based graph retrieval - hÆ°á»›ng thay tháº¿
5. Yasunaga et al. (2021): QA-GNN - truy há»“i theo node
6. Chen et al. (2024): LLMs on graphs survey - bá»‘i cáº£nh rá»™ng hÆ¡n

**Future**:
7. Theo dÃµi cÃ¡c tÃ¡i láº­p GRAG (náº¿u cÃ³) - xÃ¡c thá»±c phÃ¡t hiá»‡n
8. TÃ¬m cÃ¡c má»Ÿ rá»™ng sang tÃ¡c vá»¥ khÃ´ng QA, miá»n thá»±c
9. Theo dÃµi cÃ¡c phÃ¢n tÃ­ch lÃ½ thuyáº¿t vá» cháº¥t lÆ°á»£ng xáº¥p xá»‰

---

## Final Verdict

### Contribution: â­â­â­â­â˜† (4/5) - Máº NH

**Novel** bÃ i toÃ¡n (textual graph retrieval cho RAG)
**Innovative** giáº£i phÃ¡p (dual-view + soft pruning)
**Practical** hiá»‡u quáº£ (thá»i gian tuyáº¿n tÃ­nh)
**Reproducible** cÃ¡ch tiáº¿p cáº­n (code cÃ³ sáºµn)

### Evidence: â­â­â˜†â˜†â˜† (2/5) - Yáº¾U

**Consistent** máº«u hÃ¬nh (GRAG > baselines)
**NO** kiá»ƒm Ä‘á»‹nh thá»‘ng kÃª (lá»—i nghiÃªm trá»ng)
**NO** tá»•ng quÃ¡t hÃ³a (miá»n háº¡n cháº¿)
**Uncertain** Ä‘á»™ tin cáº­y (cÃ³ thá»ƒ khÃ´ng láº·p láº¡i)

### Impact: â­â­â­â­â˜† (4/5) - TIá»€M NÄ‚NG CAO

**Influential** Ä‘á»‹nh hÃ¬nh váº¥n Ä‘á»
**Likely** truyá»n cáº£m há»©ng cho nghiÃªn cá»©u tiáº¿p theo
**But** Ã¡p dá»¥ng thá»±c táº¿ cáº§n xÃ¡c thá»±c
**Need** cÃ¡c nghiÃªn cá»©u tÃ¡i láº­p Ä‘á»™c láº­p

---

## Bottom Line

GRAG giá»›i thiá»‡u má»™t bÃ i toÃ¡n quan trá»ng vÃ  cÃ³ Ä‘á»™ng cÆ¡ rÃµ (graph-augmented RAG) vá»›i má»™t giáº£i phÃ¡p ká»¹ thuáº­t thanh thoÃ¡t (truy há»“i chia-Ä‘á»ƒ-trá»‹ + sinh theo hai gÃ³c nhÃ¬n). CÃ¡ch tiáº¿p cáº­n nÃ y cho tháº¥y cÃ¡c máº«u hÃ¬nh há»©a háº¹n trÃªn cÃ¡c benchmark.

**Tuy nhiÃªn**, viá»‡c **hoÃ n toÃ n khÃ´ng cÃ³ kiá»ƒm Ä‘á»‹nh Ã½ nghÄ©a thá»‘ng kÃª** lÃ  má»™t lá»—i nghiÃªm trá»ng khiáº¿n cÃ¡c cáº£i thiá»‡n Ä‘Æ°á»£c tuyÃªn bá»‘ khÃ´ng thá»ƒ kiá»ƒm chá»©ng. CÃ¡c lá»£i Ã­ch khiÃªm tá»‘n so vá»›i baseline tá»‘t nháº¥t (+6%) hoÃ n toÃ n cÃ³ thá»ƒ lÃ  biáº¿n thiÃªn ngáº«u nhiÃªn náº¿u khÃ´ng cÃ³ kiá»ƒm Ä‘á»‹nh giáº£ thuyáº¿t phÃ¹ há»£p.

**Khuyáº¿n nghá»‹**:
- **Vá» Ã½ tÆ°á»Ÿng ká»¹ thuáº­t**: ÄÃ³ng gÃ³p ráº¥t giÃ¡ trá»‹ âœ“
- **Vá» tuyÃªn bá»‘ Ä‘á»‹nh lÆ°á»£ng**: Cáº§n tÃ¡i láº­p Ä‘á»™c láº­p vá»›i Ä‘á»™ nghiÃªm ngáº·t thá»‘ng kÃª âš 
- **Vá» Ã¡p dá»¥ng thá»±c táº¿**: Cáº§n phÃ¢n tÃ­ch chi phÃ­-lá»£i Ã­ch vÃ  xÃ¡c thá»±c trÃªn dá»¯ liá»‡u thá»±c âš 

**Status**: Há»©a háº¹n nhÆ°ng **chÆ°a Ä‘Æ°á»£c kiá»ƒm chá»©ng** â€” chá» cÃ¡c nghiÃªn cá»©u tÃ¡i láº­p nghiÃªm ngáº·t.

---

**Analysis completed**: 2026-01-29
**Analyst confidence**: Cao (Ä‘Ã£ báº­t cháº¿ Ä‘á»™ Ä‘á»c phÃª bÃ¬nh)
**Next steps**: Theo dÃµi tÃ¡i láº­p, má»Ÿ rá»™ng sang miá»n thá»±c táº¿

